[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the Deep Blue with Deep Learning",
    "section": "",
    "text": "Learning rates and mixed precision training\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying deep sea, reef, and freshwater fishes with a simple classifier\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/250211-cifar10/04_classification.html",
    "href": "posts/250211-cifar10/04_classification.html",
    "title": "Learning rates and mixed precision training",
    "section": "",
    "text": "Today weâ€™re teaching a neural network to distinguish images from the CIFAR-10 dataset, which contains tiny pictures of airplanes, birds, cats, dogs, and more.\nWeâ€™ll start simple, training a baseline model, and then use it as a jumping off point to play with some new concepts like discriminative learning rates and mixed precision training.\nLetâ€™s roll up our sleeves and dive in!"
  },
  {
    "objectID": "posts/250211-cifar10/04_classification.html#the-best-model",
    "href": "posts/250211-cifar10/04_classification.html#the-best-model",
    "title": "Learning rates and mixed precision training",
    "section": "The Best Model",
    "text": "The Best Model\n\n# Model D\nn_epochs = 32\ncifar = DataBlock(\n    blocks = [ImageBlock, CategoryBlock],\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42),\n    get_y = parent_label,\n    batch_tfms=aug_transforms(size=24, min_scale=0.75)\n)\ndls = cifar.dataloaders(path/'train', \n                        bs = 512, \n                        seed = 42)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlr = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbase_lr = 1.4e-3\nlearn.freeze()\nlearn.fit_one_cycle(3, base_lr)\nlearn.unfreeze()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.458459\n1.714187\n0.432300\n00:23\n\n\n1\n1.839833\n1.476324\n0.483800\n00:25\n\n\n2\n1.624287\n1.423428\n0.499400\n00:26\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\n\n\n\n\n\nbase_lr = 1.4e-3\ndisc_lr = slice(8e-5,8e-3)\nfreeze_epochs = 3\nn_epochs = 32\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.freeze()\nlearn.fit_one_cycle(freeze_epochs, base_lr)\nlearn.unfreeze()\nlearn.fit_one_cycle(n_epochs, disc_lr)\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.492095\n1.698559\n0.433000\n00:29\n\n\n1\n1.857048\n1.463014\n0.495400\n00:30\n\n\n2\n1.633801\n1.423831\n0.501000\n00:28\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.446787\n1.295929\n0.548800\n00:29\n\n\n1\n1.269360\n1.142912\n0.602900\n00:28\n\n\n2\n1.092596\n1.073082\n0.636900\n00:28\n\n\n3\n0.965740\n1.008197\n0.670700\n00:30\n\n\n4\n0.894361\n1.037594\n0.685900\n00:30\n\n\n5\n0.850987\n0.870906\n0.704600\n00:29\n\n\n6\n0.797181\n0.848414\n0.713000\n00:29\n\n\n7\n0.755061\n0.818326\n0.731000\n00:26\n\n\n8\n0.702218\n0.763084\n0.745400\n00:30\n\n\n9\n0.668742\n0.762206\n0.747800\n00:28\n\n\n10\n0.648104\n0.794793\n0.736600\n00:23\n\n\n11\n0.614646\n0.772262\n0.749800\n00:27\n\n\n12\n0.576669\n0.749846\n0.752800\n00:26\n\n\n13\n0.531431\n0.749366\n0.759000\n00:28\n\n\n14\n0.493150\n0.715264\n0.768700\n00:28\n\n\n15\n0.467971\n0.730673\n0.764300\n00:27\n\n\n16\n0.442254\n0.716524\n0.776800\n00:29\n\n\n17\n0.407482\n0.748114\n0.766100\n00:30\n\n\n18\n0.372706\n0.745389\n0.772900\n00:28\n\n\n19\n0.341517\n0.755464\n0.768600\n00:30\n\n\n20\n0.313545\n0.781643\n0.767300\n00:29\n\n\n21\n0.301854\n0.779249\n0.767500\n00:30\n\n\n22\n0.275095\n0.798806\n0.770800\n00:28\n\n\n23\n0.252724\n0.813480\n0.772600\n00:29\n\n\n24\n0.229407\n0.811392\n0.775200\n00:28\n\n\n25\n0.220920\n0.848612\n0.772700\n00:27\n\n\n26\n0.194887\n0.844323\n0.774200\n00:28\n\n\n27\n0.191182\n0.838158\n0.777200\n00:29\n\n\n28\n0.181282\n0.853957\n0.775500\n00:28\n\n\n29\n0.167093\n0.863638\n0.775000\n00:28\n\n\n30\n0.171526\n0.858613\n0.775500\n00:31\n\n\n31\n0.176508\n0.868455\n0.777200\n00:29"
  },
  {
    "objectID": "posts/250211-cifar10/04_classification.html#mixed-precision-training-resnet101",
    "href": "posts/250211-cifar10/04_classification.html#mixed-precision-training-resnet101",
    "title": "Learning rates and mixed precision training",
    "section": "Mixed Precision Training: Resnet101",
    "text": "Mixed Precision Training: Resnet101\nI decided to also test out mixed precision training to allow training of a much larger network, Resnet101, without taking too much time. The accuracy of the model actually did not improve with the deeper network. Seems like it should have, but thereâ€™s much more to do before this model is SOTA. ImageNet pretraining may not be as good as simply training from scratch, and best efforts I ran across so far did not use pretrained Imagenet models, but instead started from scratch.\n\nfrom fastai.callback.fp16 import *\n\ncifar = DataBlock(\n    blocks = [ImageBlock, CategoryBlock],\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42),\n    get_y = parent_label,\n    batch_tfms=aug_transforms(size=24, min_scale=0.75)\n)\ndls = cifar.dataloaders(path/'train', \n                        bs = 512, \n                        seed = 42)\nlearn = vision_learner(dls, resnet101, metrics=accuracy).to_fp16()\nlr = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbase_lr = 6e-3\nlearn.freeze()\nlearn.fit_one_cycle(1, base_lr)\nlearn.unfreeze()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.637301\n1.372016\n0.511300\n00:36\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00015848931798245758)\n\n\n\n\n\n\n\n\n\n\nbase_lr = 6e-3\ndisc_lr = slice(1e-5,1e-3)\nfreeze_epochs = 1\nn_epochs = 40\n\nlearn = vision_learner(dls, resnet101, metrics=accuracy).to_fp16()\nlearn.freeze()\nlearn.fit_one_cycle(freeze_epochs, base_lr)\nlearn.unfreeze()\nlearn.fit_one_cycle(n_epochs, disc_lr)\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.641111\n1.315193\n0.536400\n00:32\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.329345\n1.266843\n0.555600\n00:40\n\n\n1\n1.246262\n1.180738\n0.581500\n00:41\n\n\n2\n1.160876\n1.114313\n0.608000\n00:41\n\n\n3\n1.059085\n1.024999\n0.646800\n00:40\n\n\n4\n0.967155\n0.956626\n0.678800\n00:41\n\n\n5\n0.883919\n0.877438\n0.696900\n00:41\n\n\n6\n0.804704\n0.896085\n0.718200\n00:40\n\n\n7\n0.770523\n1.013896\n0.705100\n00:41\n\n\n8\n0.723203\n0.837037\n0.723600\n00:44\n\n\n9\n0.664702\n0.789105\n0.733400\n00:44\n\n\n10\n0.621836\n0.814080\n0.736800\n00:41\n\n\n11\n0.582255\n0.809976\n0.730500\n00:41\n\n\n12\n0.536979\n0.932742\n0.732400\n00:41\n\n\n13\n0.503126\n0.868716\n0.740800\n00:40\n\n\n14\n0.470739\n0.868617\n0.744600\n00:40\n\n\n15\n0.438311\n0.787409\n0.751000\n00:42\n\n\n16\n0.406661\n0.824242\n0.752100\n00:40\n\n\n17\n0.370056\n0.818419\n0.755500\n00:40\n\n\n18\n0.331431\n0.803667\n0.755200\n00:42\n\n\n19\n0.304569\n0.832193\n0.755100\n00:41\n\n\n20\n0.278200\n0.821361\n0.763000\n00:41\n\n\n21\n0.258393\n0.866658\n0.756300\n00:43\n\n\n22\n0.236941\n0.860195\n0.761100\n00:44\n\n\n23\n0.221599\n0.907863\n0.755600\n00:41\n\n\n24\n0.211816\n0.896679\n0.757100\n00:42\n\n\n25\n0.184622\n0.904773\n0.759300\n00:41\n\n\n26\n0.180264\n0.901700\n0.764000\n00:41\n\n\n27\n0.151746\n0.918147\n0.767000\n00:41\n\n\n28\n0.135492\n0.911483\n0.773300\n00:42\n\n\n29\n0.131715\n0.927934\n0.762800\n00:41\n\n\n30\n0.122062\n0.953696\n0.761800\n00:41\n\n\n31\n0.120269\n0.952933\n0.765100\n00:42\n\n\n32\n0.103523\n0.952526\n0.765900\n00:41\n\n\n33\n0.097716\n0.958341\n0.768900\n00:40\n\n\n34\n0.095643\n0.974640\n0.766700\n00:40\n\n\n35\n0.094067\n0.964051\n0.769500\n00:43\n\n\n36\n0.088655\n0.946297\n0.771300\n00:41\n\n\n37\n0.085030\n0.962778\n0.769100\n00:42\n\n\n38\n0.082994\n0.951210\n0.770100\n00:41\n\n\n39\n0.085635\n0.962201\n0.767800\n00:41"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "Welcome to my blog where I explore the intersection of Deep Learning and Ocean Science. As an AI enthusiast, my goal is to apply cutting-edge machine learning techniques to tackle some of the most pressing challenges related to ocean conservation, marine research, and underwater exploration.\n\n\nIâ€™m passionate about harnessing the power of AI to solve real-world problems. Through this blog, I showcase some deep learning projects that focus on the ocean, from predicting marine life populations to detecting underwater anomalies using neural networks.\n\n\n\nOn this blog, youâ€™ll find:\n\nResearch Projects: AI-driven solutions for oceanography and marine biology.\nCode Tutorials: Step-by-step guides for implementing deep learning models in ocean-related research.\nData Exploration: Insights into datasets related to marine ecosystems, underwater acoustics, and more.\nCase Studies: Real-world applications of deep learning techniques in the oceanic space.\n\n\n\n\n\nMarine Life Classification: Using Convolutional Neural Networks (CNNs) to classify marine species from underwater images.\nOcean Surface Temperature Prediction: A deep learning model to predict temperature patterns across global oceans.\nWhale Detection: Leveraging audio recognition to identify and track whale sounds from oceanic datasets.\nCoral Reef Health Monitoring: Anomaly detection using deep learning to monitor the health of coral reefs through satellite imagery.\n\n\n\n\nThe ocean covers more than 70% of the Earthâ€™s surface, yet much of it remains unexplored and understudied. With the help of AI and machine learning, we can unlock insights that help protect and preserve our oceans, marine life, and ecosystems for future generations. From predicting climate change impacts to monitoring biodiversity, the ocean is an ideal place for applying AIâ€™s transformative power.\n\n\n\nYou can also follow me on LinkedIn.\n\nThank you for visiting my blog! Dive into the world of ocean scinece AI, and letâ€™s explore the deep blue together ðŸŒŠ."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "Iâ€™m passionate about harnessing the power of AI to solve real-world problems. Through this blog, I showcase some deep learning projects that focus on the ocean, from predicting marine life populations to detecting underwater anomalies using neural networks."
  },
  {
    "objectID": "about.html#what-youll-find-here",
    "href": "about.html#what-youll-find-here",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "On this blog, youâ€™ll find:\n\nResearch Projects: AI-driven solutions for oceanography and marine biology.\nCode Tutorials: Step-by-step guides for implementing deep learning models in ocean-related research.\nData Exploration: Insights into datasets related to marine ecosystems, underwater acoustics, and more.\nCase Studies: Real-world applications of deep learning techniques in the oceanic space."
  },
  {
    "objectID": "about.html#featured-projects",
    "href": "about.html#featured-projects",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "Marine Life Classification: Using Convolutional Neural Networks (CNNs) to classify marine species from underwater images.\nOcean Surface Temperature Prediction: A deep learning model to predict temperature patterns across global oceans.\nWhale Detection: Leveraging audio recognition to identify and track whale sounds from oceanic datasets.\nCoral Reef Health Monitoring: Anomaly detection using deep learning to monitor the health of coral reefs through satellite imagery."
  },
  {
    "objectID": "about.html#why-the-ocean",
    "href": "about.html#why-the-ocean",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "The ocean covers more than 70% of the Earthâ€™s surface, yet much of it remains unexplored and understudied. With the help of AI and machine learning, we can unlock insights that help protect and preserve our oceans, marine life, and ecosystems for future generations. From predicting climate change impacts to monitoring biodiversity, the ocean is an ideal place for applying AIâ€™s transformative power."
  },
  {
    "objectID": "about.html#stay-updated",
    "href": "about.html#stay-updated",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "You can also follow me on LinkedIn.\n\nThank you for visiting my blog! Dive into the world of ocean scinece AI, and letâ€™s explore the deep blue together ðŸŒŠ."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html",
    "href": "posts/250203-fish-types/03_fish_types.html",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "",
    "text": "Today, weâ€™re going to build something fast and easy, and check out a way to quickly compile and clean a dataset gathered from the Bing Image Search API. This fish classifier is built using Pytorch and FastAI, and reaches 93% accuracy using a very small dataset, only 380 images in total. This classifier can differentiate between fish that belong on coral reefs, freshwater, or in the deep ocean. Letâ€™s dive in!"
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#pre-planning",
    "href": "posts/250203-fish-types/03_fish_types.html#pre-planning",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Pre-planning",
    "text": "Pre-planning\nBefore we dive straight in to model building, letâ€™s think like an engineer. What do we actually want to accomplish?\n\nDefine the objective â€“ We want to build a model that can classify images into deep sea, reef, or freshwater fish. Not just the ones in our dataset, but any fish picture we throw at it.\nWhat actions can we take? â€“ We can gather a dataset of fish images, clean it up so itâ€™s not full of junk, and train a model to be as accurate as possible.\nWhat data do we have? â€“ The internet is full of fish pictures! Weâ€™ll use Bingâ€™s image search API to scrape some and build our own dataset."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#gathering-a-dataset",
    "href": "posts/250203-fish-types/03_fish_types.html#gathering-a-dataset",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Gathering a Dataset",
    "text": "Gathering a Dataset\nFirst, letâ€™s grab some fish pictures from Bing.\n\nfrom fastai.vision.all import *\n\n\nkey = os.environ.get('AZURE_SEARCH_KEY', 'my_api_key')  # insert key value here\npath = Path(\"fish\")\n\nSEARCH_TERMS = [\"deep sea fish\", \"freshwater fish\", \"reef fish\"]\nfor o in SEARCH_TERMS:\n    dest = path/o\n    if not os.path.exists(dest):\n        os.makedirs(dest)\n        results = search_images_bing(key, o)\n        download_images(dest, urls=results.attrgot('contentUrl')) # dest is a path object \n\nGreat, now we have folders full of images. But the internet is messyâ€” some of these are probably not fish, some might be mislabeled, and some might be totally useless. We need to clean up."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#cleaning-the-dataset",
    "href": "posts/250203-fish-types/03_fish_types.html#cleaning-the-dataset",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Cleaning the dataset",
    "text": "Cleaning the dataset\nInstead of going through them by hand (boring! slow!), weâ€™ll train a quick classifier to help us sort out the bad ones. First we can remove any obviously broken files.\n\nfns = get_image_files(path)  # finds all image files in path and subpaths\nfailed = verify_images(fns)\nfailed.map(Path.unlink)\n\nThen train a quick model to help us clean the rest.\n\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(224, min_scale=0.5),\n                                      batch_tfms=aug_transforms())\n\n\ndls.valid.show_batch(max_n=5, nrows=1)\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\n\n\n\n\n\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.648003\n0.747909\n0.671053\n00:18\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.796192\n0.540283\n0.776316\n00:16\n\n\n1\n0.682664\n0.470164\n0.815789\n00:16\n\n\n2\n0.550159\n0.427897\n0.828947\n00:16\n\n\n\n\n\nLetâ€™s use this model to find the images itâ€™s most confused about. Those images are likely misclassified or just bad images.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(10, nrows=4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom fastai.vision.widgets import *\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis pulls up an interactive widget where we can delete the bad images or move them to the correct category.\n\n\n\nImageClassifierCleaner\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nBoom! Dataset cleaned. Now we can train the real model."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#experimenting-with-the-model",
    "href": "posts/250203-fish-types/03_fish_types.html#experimenting-with-the-model",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Experimenting with the model",
    "text": "Experimenting with the model\nNow that we have a solid dataset, we can first build a baseline model and then experiment with hyperparameters.\n\nBaseline Model\n\n### Experiment 1: Baseline\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(224, min_scale=0.5),\n                                      batch_tfms=aug_transforms())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.110603\n0.928864\n0.636364\n00:14\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.876281\n0.530159\n0.779221\n00:14\n\n\n1\n0.707189\n0.526176\n0.831169\n00:12\n\n\n2\n0.625902\n0.444526\n0.857143\n00:12\n\n\n\n\n\nThis gives us a good starting point, but we can do better!\n\n\nLarger Image Size\nMaybe the model just needs to see more details in the fish.\n\n### Experiment 2: Larger images\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(500, min_scale=0.5),\n                                      batch_tfms=aug_transforms())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.968835\n0.604990\n0.792208\n00:18\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.743671\n0.430460\n0.818182\n00:19\n\n\n1\n0.598641\n0.391256\n0.857143\n00:18\n\n\n2\n0.471975\n0.371211\n0.857143\n00:18\n\n\n\n\n\nContinuing on, we can play other hyperparameters like with min_scale value, try some deeper or different model architectures, and finally train to overfitting to discover the best number of epochs to train for. Once weâ€™re happy, we save our model for future use."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#save-the-final-model",
    "href": "posts/250203-fish-types/03_fish_types.html#save-the-final-model",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Save the final model",
    "text": "Save the final model\n\n### Final Model\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(500, min_scale=0.75),\n                                      batch_tfms=aug_transforms())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(5)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.615644\n0.802557\n0.644737\n00:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.800859\n0.560063\n0.736842\n00:21\n\n\n1\n0.667470\n0.369439\n0.828947\n00:21\n\n\n2\n0.520393\n0.288026\n0.907895\n00:22\n\n\n3\n0.423325\n0.282853\n0.921053\n00:20\n\n\n4\n0.363907\n0.296785\n0.934211\n00:20\n\n\n\n\n\n\nlearn.export()\n\nWe cleaned the dataset and gained significantly in accuracy, hitting 93% on a model trained on only 380 images. If we choose, we can gain rapidly in accuracy by spending a bit more time adding to and continuing to clean the dataset."
  }
]