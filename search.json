[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the Deep Blue with Deep Learning",
    "section": "",
    "text": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 13, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\n\n\n\n\n\n\nLearning rates and mixed precision training\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying deep sea, reef, and freshwater fishes with a simple classifier\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "",
    "text": "Today, I want to show you how to take on the classic CIFAR-10 image classification problem and outmaneuver the top-published ResNet-18 models on the Papers with Code Leaderboard. CIFAR-10 is a widely beloved dataset of 32×32 color images across ten classes (cats, dogs, frogs, ships, airplanes, etc.). Because of its modest size, it’s a perfect playground to mix and match advanced tricks—and see how far we can push a standard ResNet-18.\nLet’s roll up our sleeves and not be afraid to experiment. We’ll do a bit of “what if” poking around until we find the best approach. By the end, you’ll see how easy it is to break new ground with the right techniques."
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html#inspect-the-dataset",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html#inspect-the-dataset",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "Inspect the dataset",
    "text": "Inspect the dataset\nHere, we’ll look at the images, do some basic transformations (random augmentations, normalization, resizing), and then check that our data is normalized around zero mean and unit standard deviation. This normalizing step is typically crucial to help models train consistently.\n\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_20pct_files,\n                   get_y=parent_label,\n                   splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                   batch_tfms=[*aug_transforms(size=24, min_scale=0.75),\n                               Normalize.from_stats(*cifar_stats)])\n\n\ndls = dblock.dataloaders(path/'train', bs=512, seed=42)\n\nWe then do a quick mean/std check: if the mean is close to zero and std is close to one, we know our normalization is in good shape.\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n(TensorImage([ 0.0049, -0.0181, -0.0277], device='cuda:0'),\n TensorImage([0.9488, 0.9568, 0.9518], device='cuda:0'))"
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html#helper-functions-for-dataloaders",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html#helper-functions-for-dataloaders",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "Helper Functions for DataLoaders",
    "text": "Helper Functions for DataLoaders\nWe’ll define a few variants of get_dls(…) so we can try different transformations quickly—sometimes with normalization, sometimes without, sometimes with more or fewer augmentations.\n\n\nCode\ndef get_dls(bs, size, aug_size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_20pct_files,\n                   get_y=parent_label,\n                   splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                   item_tfms=Resize(size),\n                   batch_tfms=[*aug_transforms(size=aug_size, min_scale=0.75),\n                               Normalize.from_stats(*cifar_stats)])\n    return dblock.dataloaders(path/'train', bs=bs, seed=42)\n\ndef get_dls_no_norm(bs, size, aug_size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       get_items=get_20pct_files,\n                       get_y=parent_label,\n                       splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                       item_tfms=Resize(size),\n                       batch_tfms=aug_transforms(size=aug_size, min_scale=0.75))\n    return dblock.dataloaders(path/'train', bs=bs, seed=42)\n\ndef get_dls_no_norm_no_aug(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       get_items=get_20pct_files,\n                       get_y=parent_label,\n                       splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                       item_tfms=Resize(size))\n    return dblock.dataloaders(path/'train', bs=bs, seed=42)\n\ndef get_dls_no_aug(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       get_items=get_20pct_files,\n                       get_y=parent_label,\n                       splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                       item_tfms=Resize(size),\n                       batch_tfms=Normalize.from_stats(*cifar_stats))\n    return dblock.dataloaders(path/'train', bs=bs, seed=42)\n\n\nLooking at a batch of CIFAR-10 data, you see it’s 32x32 pixel color images. Many of these are hard for me to pick a label for, so a 96% success rate by the model might even outperform my human capability.\n\ndls.valid.show_batch(max_n=12, nrows=3, ncols=4)\n\n\n\n\n\n\n\n\n\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n\n(torch.Size([512, 3, 24, 24]), torch.Size([512]))"
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html#baseline-model",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html#baseline-model",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "Baseline Model",
    "text": "Baseline Model\n47.8% Accuracy\nHere’s our baseline training—no fancy bells or whistles, just to see where we stand. I’ll show the code; normally we’d let it run and check the final accuracy. a\n\n# Baseline Model\nn_epochs=32\nlearn.fit(n_epochs)\n\nThe baseline might hover around 47–50% accuracy. That’s a simple reference point—like measuring the “un-augmented” skill of the model. We’ll now run a bunch of experiments, each time reloading the data with different transforms or training hyperparameters, and noting the final accuracy. (I’ve trimmed the epoch-by-epoch logs for brevity.)"
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html#model-experiments",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html#model-experiments",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "Model Experiments",
    "text": "Model Experiments\nLeader: Model D2\nAccuracy: 93.6%\n\nModel Z - Original size, resnet18\nAccuracy ~ 65.6%. Just by using original 32×32 images with minimal augmentation, we get about two-thirds accuracy.\n\n# Model Z\ndls = get_dls(512, 32, 24)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(10, base_lr=2e-2, freeze_epochs=3)\n\n\n\nModel Z2 - Original size, smaller bs, resnet18\nAccuracy ~ 70.9%. Smaller batch sizes sometimes help or hurt, depending on GPU memory or other hyperparameter interactions. We get a boost here.\n\n# Model Z2\ndls = get_dls(64, 32, 24)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(10, base_lr=2e-2, freeze_epochs=3)\n\n\n\nModel A - Size 128, resnet18, small bs\nWe scale images to 128×128, do a bit more augmentation, and see what happens:\nAccuracy ~ 91.3%. A big jump—resizing those tiny CIFAR-10 images to 128×128 and applying more data augmentation helps the model see more varied data.\n\n## Model A\ndls = get_dls(64, 128, 96)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlr = learn.lr_find()\n\n\nlr = 3e-3\nn_epochs = 32\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel A2 - Size 128, resnet18, small bs, more freeze_epochs\nAccuracy ~ 90.9%. Freezing the early layers a bit longer sometimes changes the final outcome. In this run, it’s slightly less than 91.3%.\n\n# Model A2\ndls = get_dls(64, 128, 96)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlr = 3e-3\nn_epochs = 32\nlearn.fine_tune(n_epochs, base_lr=lr, freeze_epochs=3)\n\n\n\nModel B- Size 128, resnet18, small bs, no norm\nNo normalization to see how important standardizing the data might be. Accuracy ~ 90.2%. Still respectable, but generally standardization is beneficial.\n\n# Model B\ndls = get_dls_no_norm(64, 128, 96)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\n#learn.lr_find()\n\n\nlr = 3e-3\nn_epochs = 32\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel C- Size 128, resnet18, small bs, no aug\nAccuracy ~ 91.0%. So even with no random augmentations (but still normalized), we break 90%.\n\n# Model C\ndls = get_dls_no_aug(64, 128)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.lr_find()\n\n\nlr = 3e-3\nn_epochs = 32\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel C3- Size 256, resnet18, small bs\nLet’s go bigger still—256×256. With bigger images, the model sees more details: Accuracy: 92.0%\n\n# Model C3\ndls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlr = learn.lr_find()\n\n\nlr = 3e-3\nn_epochs = 32\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel D - Mixup and Label Smoothing\nAccuracy ~ 93.5%. This is a hefty leap.\nTwo powerful techniques:\nMixUp: randomly blends two images and their labels, forcing the model not to rely on too-literal image cues. Label Smoothing: slightly softens the one-hot labels, mitigating overconfidence.\n\n# Model D - Mixup and Label Smoothing\ndls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.lr_find()\n\n\nlr=3e-3\nn_epochs=32\nlearn = vision_learner(dls, resnet18, loss_func=LabelSmoothingCrossEntropy(),\n                metrics=accuracy, cbs=MixUp())\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel E0 - Progressive Resizing\nAccuracy: 90.0%\nWe train first on smaller images, then gradually increase their size. This can help if you’re using a pretrained model so it first learns broad features, then refines them with bigger images. But on CIFAR-10, which is already small, it’s not always beneficial.\n\n# Model E0 - Progressive Resizing\nn_epochs = 16\nlr = 3e-3\n\ndls = get_dls(64, 128, 96)\nlearn = vision_learner(dls, resnet18, pretrained=False, loss_func=LabelSmoothingCrossEntropy(),\n                metrics=accuracy, cbs=MixUp())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\nlearn.dls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel F- Discriminative Learning Rates\nAccuracy: 90.8%\nThe idea: later layers can use higher learning rates while earlier, more general layers use a lower one. In fastai, we do this with a range (slice(lr/10, lr*10)).\n\n# Model F\nlr=3e-3\nn_epochs=32\ndls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, loss_func=LabelSmoothingCrossEntropy(),\n                metrics=accuracy, cbs=MixUp())\nlearn.fit_one_cycle(1, lr_max=lr)\nlearn.lr_find()\n\n\nlr=2e-3\nlearn.fit_one_cycle(n_epochs, lr_max=slice(lr/10,lr*10))\n\n\n\nModel D2- Mixup and Label Smoothing, Mixed Point Precision\nAccuracy: 93.6%\nWe add one more trick: Mixed-precision training (a.k.a. fp16). It runs your forward/backward passes in half precision on the GPU, speeding things up and sometimes improving generalization.\n\n# Model D Mixed Point\ndls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.lr_find()\nlr=3e-3\nn_epochs=32\nlearn = vision_learner(dls, resnet18, loss_func=LabelSmoothingCrossEntropy(),\n                metrics=accuracy, cbs=MixUp()).to_fp16()\nlearn.fine_tune(n_epochs, base_lr=lr)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "Welcome to my blog where I explore the intersection of Deep Learning and Ocean Science. As an AI enthusiast, my goal is to apply cutting-edge machine learning techniques to tackle some of the most pressing challenges related to ocean conservation, marine research, and underwater exploration.\n\n\nI’m passionate about harnessing the power of AI to solve real-world problems. Through this blog, I showcase some deep learning projects that focus on the ocean, from predicting marine life populations to detecting underwater anomalies using neural networks.\n\n\n\nOn this blog, you’ll find:\n\nResearch Projects: AI-driven solutions for oceanography and marine biology.\nCode Tutorials: Step-by-step guides for implementing deep learning models in ocean-related research.\nData Exploration: Insights into datasets related to marine ecosystems, underwater acoustics, and more.\nCase Studies: Real-world applications of deep learning techniques in the oceanic space.\n\n\n\n\n\nMarine Life Classification: Using Convolutional Neural Networks (CNNs) to classify marine species from underwater images.\nOcean Surface Temperature Prediction: A deep learning model to predict temperature patterns across global oceans.\nWhale Detection: Leveraging audio recognition to identify and track whale sounds from oceanic datasets.\nCoral Reef Health Monitoring: Anomaly detection using deep learning to monitor the health of coral reefs through satellite imagery.\n\n\n\n\nThe ocean covers more than 70% of the Earth’s surface, yet much of it remains unexplored and understudied. With the help of AI and machine learning, we can unlock insights that help protect and preserve our oceans, marine life, and ecosystems for future generations. From predicting climate change impacts to monitoring biodiversity, the ocean is an ideal place for applying AI’s transformative power.\n\n\n\nYou can also follow me on LinkedIn.\n\nThank you for visiting my blog! Dive into the world of ocean scinece AI, and let’s explore the deep blue together 🌊."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "I’m passionate about harnessing the power of AI to solve real-world problems. Through this blog, I showcase some deep learning projects that focus on the ocean, from predicting marine life populations to detecting underwater anomalies using neural networks."
  },
  {
    "objectID": "about.html#what-youll-find-here",
    "href": "about.html#what-youll-find-here",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "On this blog, you’ll find:\n\nResearch Projects: AI-driven solutions for oceanography and marine biology.\nCode Tutorials: Step-by-step guides for implementing deep learning models in ocean-related research.\nData Exploration: Insights into datasets related to marine ecosystems, underwater acoustics, and more.\nCase Studies: Real-world applications of deep learning techniques in the oceanic space."
  },
  {
    "objectID": "about.html#featured-projects",
    "href": "about.html#featured-projects",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "Marine Life Classification: Using Convolutional Neural Networks (CNNs) to classify marine species from underwater images.\nOcean Surface Temperature Prediction: A deep learning model to predict temperature patterns across global oceans.\nWhale Detection: Leveraging audio recognition to identify and track whale sounds from oceanic datasets.\nCoral Reef Health Monitoring: Anomaly detection using deep learning to monitor the health of coral reefs through satellite imagery."
  },
  {
    "objectID": "about.html#why-the-ocean",
    "href": "about.html#why-the-ocean",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "The ocean covers more than 70% of the Earth’s surface, yet much of it remains unexplored and understudied. With the help of AI and machine learning, we can unlock insights that help protect and preserve our oceans, marine life, and ecosystems for future generations. From predicting climate change impacts to monitoring biodiversity, the ocean is an ideal place for applying AI’s transformative power."
  },
  {
    "objectID": "about.html#stay-updated",
    "href": "about.html#stay-updated",
    "title": "Lindy Rauchenstein",
    "section": "",
    "text": "You can also follow me on LinkedIn.\n\nThank you for visiting my blog! Dive into the world of ocean scinece AI, and let’s explore the deep blue together 🌊."
  },
  {
    "objectID": "posts/250211-cifar10/04_classification.html",
    "href": "posts/250211-cifar10/04_classification.html",
    "title": "Learning rates and mixed precision training",
    "section": "",
    "text": "Today we’re teaching a neural network to distinguish images from the CIFAR-10 dataset, which contains tiny pictures of airplanes, birds, cats, dogs, and more.\nWe’ll start simple, training a baseline model, and then use it as a jumping off point to play with some new concepts like discriminative learning rates and mixed precision training.\nLet’s roll up our sleeves and dive in!"
  },
  {
    "objectID": "posts/250211-cifar10/04_classification.html#the-best-model",
    "href": "posts/250211-cifar10/04_classification.html#the-best-model",
    "title": "Learning rates and mixed precision training",
    "section": "The Best Model",
    "text": "The Best Model\n\n# Model D\nn_epochs = 32\ncifar = DataBlock(\n    blocks = [ImageBlock, CategoryBlock],\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42),\n    get_y = parent_label,\n    batch_tfms=aug_transforms(size=24, min_scale=0.75)\n)\ndls = cifar.dataloaders(path/'train', \n                        bs = 512, \n                        seed = 42)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlr = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbase_lr = 1.4e-3\nlearn.freeze()\nlearn.fit_one_cycle(3, base_lr)\nlearn.unfreeze()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.458459\n1.714187\n0.432300\n00:23\n\n\n1\n1.839833\n1.476324\n0.483800\n00:25\n\n\n2\n1.624287\n1.423428\n0.499400\n00:26\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\n\n\n\n\n\nbase_lr = 1.4e-3\ndisc_lr = slice(8e-5,8e-3)\nfreeze_epochs = 3\nn_epochs = 32\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.freeze()\nlearn.fit_one_cycle(freeze_epochs, base_lr)\nlearn.unfreeze()\nlearn.fit_one_cycle(n_epochs, disc_lr)\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.492095\n1.698559\n0.433000\n00:29\n\n\n1\n1.857048\n1.463014\n0.495400\n00:30\n\n\n2\n1.633801\n1.423831\n0.501000\n00:28\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.446787\n1.295929\n0.548800\n00:29\n\n\n1\n1.269360\n1.142912\n0.602900\n00:28\n\n\n2\n1.092596\n1.073082\n0.636900\n00:28\n\n\n3\n0.965740\n1.008197\n0.670700\n00:30\n\n\n4\n0.894361\n1.037594\n0.685900\n00:30\n\n\n5\n0.850987\n0.870906\n0.704600\n00:29\n\n\n6\n0.797181\n0.848414\n0.713000\n00:29\n\n\n7\n0.755061\n0.818326\n0.731000\n00:26\n\n\n8\n0.702218\n0.763084\n0.745400\n00:30\n\n\n9\n0.668742\n0.762206\n0.747800\n00:28\n\n\n10\n0.648104\n0.794793\n0.736600\n00:23\n\n\n11\n0.614646\n0.772262\n0.749800\n00:27\n\n\n12\n0.576669\n0.749846\n0.752800\n00:26\n\n\n13\n0.531431\n0.749366\n0.759000\n00:28\n\n\n14\n0.493150\n0.715264\n0.768700\n00:28\n\n\n15\n0.467971\n0.730673\n0.764300\n00:27\n\n\n16\n0.442254\n0.716524\n0.776800\n00:29\n\n\n17\n0.407482\n0.748114\n0.766100\n00:30\n\n\n18\n0.372706\n0.745389\n0.772900\n00:28\n\n\n19\n0.341517\n0.755464\n0.768600\n00:30\n\n\n20\n0.313545\n0.781643\n0.767300\n00:29\n\n\n21\n0.301854\n0.779249\n0.767500\n00:30\n\n\n22\n0.275095\n0.798806\n0.770800\n00:28\n\n\n23\n0.252724\n0.813480\n0.772600\n00:29\n\n\n24\n0.229407\n0.811392\n0.775200\n00:28\n\n\n25\n0.220920\n0.848612\n0.772700\n00:27\n\n\n26\n0.194887\n0.844323\n0.774200\n00:28\n\n\n27\n0.191182\n0.838158\n0.777200\n00:29\n\n\n28\n0.181282\n0.853957\n0.775500\n00:28\n\n\n29\n0.167093\n0.863638\n0.775000\n00:28\n\n\n30\n0.171526\n0.858613\n0.775500\n00:31\n\n\n31\n0.176508\n0.868455\n0.777200\n00:29"
  },
  {
    "objectID": "posts/250211-cifar10/04_classification.html#mixed-precision-training-resnet101",
    "href": "posts/250211-cifar10/04_classification.html#mixed-precision-training-resnet101",
    "title": "Learning rates and mixed precision training",
    "section": "Mixed Precision Training: Resnet101",
    "text": "Mixed Precision Training: Resnet101\nI decided to also test out mixed precision training to allow training of a much larger network, Resnet101, without taking too much time. The accuracy of the model actually did not improve with the deeper network. Seems like it should have, but there’s much more to do before this model is SOTA. ImageNet pretraining may not be as good as simply training from scratch, and best efforts I ran across so far did not use pretrained Imagenet models, but instead started from scratch.\n\nfrom fastai.callback.fp16 import *\n\ncifar = DataBlock(\n    blocks = [ImageBlock, CategoryBlock],\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42),\n    get_y = parent_label,\n    batch_tfms=aug_transforms(size=24, min_scale=0.75)\n)\ndls = cifar.dataloaders(path/'train', \n                        bs = 512, \n                        seed = 42)\nlearn = vision_learner(dls, resnet101, metrics=accuracy).to_fp16()\nlr = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbase_lr = 6e-3\nlearn.freeze()\nlearn.fit_one_cycle(1, base_lr)\nlearn.unfreeze()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.637301\n1.372016\n0.511300\n00:36\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00015848931798245758)\n\n\n\n\n\n\n\n\n\n\nbase_lr = 6e-3\ndisc_lr = slice(1e-5,1e-3)\nfreeze_epochs = 1\nn_epochs = 40\n\nlearn = vision_learner(dls, resnet101, metrics=accuracy).to_fp16()\nlearn.freeze()\nlearn.fit_one_cycle(freeze_epochs, base_lr)\nlearn.unfreeze()\nlearn.fit_one_cycle(n_epochs, disc_lr)\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.641111\n1.315193\n0.536400\n00:32\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.329345\n1.266843\n0.555600\n00:40\n\n\n1\n1.246262\n1.180738\n0.581500\n00:41\n\n\n2\n1.160876\n1.114313\n0.608000\n00:41\n\n\n3\n1.059085\n1.024999\n0.646800\n00:40\n\n\n4\n0.967155\n0.956626\n0.678800\n00:41\n\n\n5\n0.883919\n0.877438\n0.696900\n00:41\n\n\n6\n0.804704\n0.896085\n0.718200\n00:40\n\n\n7\n0.770523\n1.013896\n0.705100\n00:41\n\n\n8\n0.723203\n0.837037\n0.723600\n00:44\n\n\n9\n0.664702\n0.789105\n0.733400\n00:44\n\n\n10\n0.621836\n0.814080\n0.736800\n00:41\n\n\n11\n0.582255\n0.809976\n0.730500\n00:41\n\n\n12\n0.536979\n0.932742\n0.732400\n00:41\n\n\n13\n0.503126\n0.868716\n0.740800\n00:40\n\n\n14\n0.470739\n0.868617\n0.744600\n00:40\n\n\n15\n0.438311\n0.787409\n0.751000\n00:42\n\n\n16\n0.406661\n0.824242\n0.752100\n00:40\n\n\n17\n0.370056\n0.818419\n0.755500\n00:40\n\n\n18\n0.331431\n0.803667\n0.755200\n00:42\n\n\n19\n0.304569\n0.832193\n0.755100\n00:41\n\n\n20\n0.278200\n0.821361\n0.763000\n00:41\n\n\n21\n0.258393\n0.866658\n0.756300\n00:43\n\n\n22\n0.236941\n0.860195\n0.761100\n00:44\n\n\n23\n0.221599\n0.907863\n0.755600\n00:41\n\n\n24\n0.211816\n0.896679\n0.757100\n00:42\n\n\n25\n0.184622\n0.904773\n0.759300\n00:41\n\n\n26\n0.180264\n0.901700\n0.764000\n00:41\n\n\n27\n0.151746\n0.918147\n0.767000\n00:41\n\n\n28\n0.135492\n0.911483\n0.773300\n00:42\n\n\n29\n0.131715\n0.927934\n0.762800\n00:41\n\n\n30\n0.122062\n0.953696\n0.761800\n00:41\n\n\n31\n0.120269\n0.952933\n0.765100\n00:42\n\n\n32\n0.103523\n0.952526\n0.765900\n00:41\n\n\n33\n0.097716\n0.958341\n0.768900\n00:40\n\n\n34\n0.095643\n0.974640\n0.766700\n00:40\n\n\n35\n0.094067\n0.964051\n0.769500\n00:43\n\n\n36\n0.088655\n0.946297\n0.771300\n00:41\n\n\n37\n0.085030\n0.962778\n0.769100\n00:42\n\n\n38\n0.082994\n0.951210\n0.770100\n00:41\n\n\n39\n0.085635\n0.962201\n0.767800\n00:41"
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html",
    "href": "posts/250203-fish-types/03_fish_types.html",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "",
    "text": "Today, we’re going to build something fast and easy, and check out a way to quickly compile and clean a dataset gathered from the Bing Image Search API. This fish classifier is built using Pytorch and FastAI, and reaches 93% accuracy using a very small dataset, only 380 images in total. This classifier can differentiate between fish that belong on coral reefs, freshwater, or in the deep ocean. Let’s dive in!"
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#pre-planning",
    "href": "posts/250203-fish-types/03_fish_types.html#pre-planning",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Pre-planning",
    "text": "Pre-planning\nBefore we dive straight in to model building, let’s think like an engineer. What do we actually want to accomplish?\n\nDefine the objective – We want to build a model that can classify images into deep sea, reef, or freshwater fish. Not just the ones in our dataset, but any fish picture we throw at it.\nWhat actions can we take? – We can gather a dataset of fish images, clean it up so it’s not full of junk, and train a model to be as accurate as possible.\nWhat data do we have? – The internet is full of fish pictures! We’ll use Bing’s image search API to scrape some and build our own dataset."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#gathering-a-dataset",
    "href": "posts/250203-fish-types/03_fish_types.html#gathering-a-dataset",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Gathering a Dataset",
    "text": "Gathering a Dataset\nFirst, let’s grab some fish pictures from Bing.\n\nfrom fastai.vision.all import *\n\n\nkey = os.environ.get('AZURE_SEARCH_KEY', 'my_api_key')  # insert key value here\npath = Path(\"fish\")\n\nSEARCH_TERMS = [\"deep sea fish\", \"freshwater fish\", \"reef fish\"]\nfor o in SEARCH_TERMS:\n    dest = path/o\n    if not os.path.exists(dest):\n        os.makedirs(dest)\n        results = search_images_bing(key, o)\n        download_images(dest, urls=results.attrgot('contentUrl')) # dest is a path object \n\nGreat, now we have folders full of images. But the internet is messy— some of these are probably not fish, some might be mislabeled, and some might be totally useless. We need to clean up."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#cleaning-the-dataset",
    "href": "posts/250203-fish-types/03_fish_types.html#cleaning-the-dataset",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Cleaning the dataset",
    "text": "Cleaning the dataset\nInstead of going through them by hand (boring! slow!), we’ll train a quick classifier to help us sort out the bad ones. First we can remove any obviously broken files.\n\nfns = get_image_files(path)  # finds all image files in path and subpaths\nfailed = verify_images(fns)\nfailed.map(Path.unlink)\n\nThen train a quick model to help us clean the rest.\n\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(224, min_scale=0.5),\n                                      batch_tfms=aug_transforms())\n\n\ndls.valid.show_batch(max_n=5, nrows=1)\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\n\n\n\n\n\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.648003\n0.747909\n0.671053\n00:18\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.796192\n0.540283\n0.776316\n00:16\n\n\n1\n0.682664\n0.470164\n0.815789\n00:16\n\n\n2\n0.550159\n0.427897\n0.828947\n00:16\n\n\n\n\n\nLet’s use this model to find the images it’s most confused about. Those images are likely misclassified or just bad images.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(10, nrows=4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom fastai.vision.widgets import *\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis pulls up an interactive widget where we can delete the bad images or move them to the correct category.\n\n\n\nImageClassifierCleaner\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nBoom! Dataset cleaned. Now we can train the real model."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#experimenting-with-the-model",
    "href": "posts/250203-fish-types/03_fish_types.html#experimenting-with-the-model",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Experimenting with the model",
    "text": "Experimenting with the model\nNow that we have a solid dataset, we can first build a baseline model and then experiment with hyperparameters.\n\nBaseline Model\n\n### Experiment 1: Baseline\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(224, min_scale=0.5),\n                                      batch_tfms=aug_transforms())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.110603\n0.928864\n0.636364\n00:14\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.876281\n0.530159\n0.779221\n00:14\n\n\n1\n0.707189\n0.526176\n0.831169\n00:12\n\n\n2\n0.625902\n0.444526\n0.857143\n00:12\n\n\n\n\n\nThis gives us a good starting point, but we can do better!\n\n\nLarger Image Size\nMaybe the model just needs to see more details in the fish.\n\n### Experiment 2: Larger images\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(500, min_scale=0.5),\n                                      batch_tfms=aug_transforms())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.968835\n0.604990\n0.792208\n00:18\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.743671\n0.430460\n0.818182\n00:19\n\n\n1\n0.598641\n0.391256\n0.857143\n00:18\n\n\n2\n0.471975\n0.371211\n0.857143\n00:18\n\n\n\n\n\nContinuing on, we can play other hyperparameters like with min_scale value, try some deeper or different model architectures, and finally train to overfitting to discover the best number of epochs to train for. Once we’re happy, we save our model for future use."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#save-the-final-model",
    "href": "posts/250203-fish-types/03_fish_types.html#save-the-final-model",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Save the final model",
    "text": "Save the final model\n\n### Final Model\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(500, min_scale=0.75),\n                                      batch_tfms=aug_transforms())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(5)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.615644\n0.802557\n0.644737\n00:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.800859\n0.560063\n0.736842\n00:21\n\n\n1\n0.667470\n0.369439\n0.828947\n00:21\n\n\n2\n0.520393\n0.288026\n0.907895\n00:22\n\n\n3\n0.423325\n0.282853\n0.921053\n00:20\n\n\n4\n0.363907\n0.296785\n0.934211\n00:20\n\n\n\n\n\n\nlearn.export()\n\nWe cleaned the dataset and gained significantly in accuracy, hitting 93% on a model trained on only 380 images. If we choose, we can gain rapidly in accuracy by spending a bit more time adding to and continuing to clean the dataset."
  }
]