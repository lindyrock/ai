[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog where I explore the intersection of Deep Learning and Ocean Science. As an AI enthusiast, my goal is to apply cutting-edge machine learning techniques to tackle some of the most pressing challenges related to ocean conservation, marine research, and underwater exploration.\n\n\nThe ocean covers more than 70% of the Earth’s surface, yet much of it remains unexplored and understudied. With the help of AI and machine learning, we can unlock insights that help protect and preserve our oceans, marine life, and ecosystems for future generations. From predicting climate change impacts to monitoring biodiversity, the ocean is an ideal place for applying AI’s transformative power.\n\n\n\nI’m passionate about harnessing the power of AI to solve real-world problems. Through this blog, I showcase some deep learning projects that focus on the ocean, from predicting marine life populations to detecting underwater anomalies using neural networks.\n\n\n\nOn this blog, you’ll find:\n\nResearch Projects: AI-driven solutions for oceanography and marine biology.\nCode Tutorials: Step-by-step guides for implementing deep learning models in ocean-related research.\nData Exploration: Insights into datasets related to marine ecosystems, underwater acoustics, and more.\nCase Studies: Real-world applications of deep learning techniques in the oceanic space.\n\n\n\n\n\nMarine Life Classification: Using Convolutional Neural Networks (CNNs) to classify marine species from underwater images.\nOcean Surface Temperature Prediction: A deep learning model to predict temperature patterns across global oceans.\nWhale Detection: Leveraging audio recognition to identify and track whale sounds from oceanic datasets.\nCoral Reef Health Monitoring: Anomaly detection using deep learning to monitor the health of coral reefs through satellite imagery.\n\n\n\n\nYou can also follow me on LinkedIn.\n\nThank you for visiting my blog! Dive into the world of ocean scinece AI, and let’s explore the deep blue together 🌊."
  },
  {
    "objectID": "about.html#why-the-ocean",
    "href": "about.html#why-the-ocean",
    "title": "About",
    "section": "",
    "text": "The ocean covers more than 70% of the Earth’s surface, yet much of it remains unexplored and understudied. With the help of AI and machine learning, we can unlock insights that help protect and preserve our oceans, marine life, and ecosystems for future generations. From predicting climate change impacts to monitoring biodiversity, the ocean is an ideal place for applying AI’s transformative power."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "I’m passionate about harnessing the power of AI to solve real-world problems. Through this blog, I showcase some deep learning projects that focus on the ocean, from predicting marine life populations to detecting underwater anomalies using neural networks."
  },
  {
    "objectID": "about.html#what-youll-find-here",
    "href": "about.html#what-youll-find-here",
    "title": "About",
    "section": "",
    "text": "On this blog, you’ll find:\n\nResearch Projects: AI-driven solutions for oceanography and marine biology.\nCode Tutorials: Step-by-step guides for implementing deep learning models in ocean-related research.\nData Exploration: Insights into datasets related to marine ecosystems, underwater acoustics, and more.\nCase Studies: Real-world applications of deep learning techniques in the oceanic space."
  },
  {
    "objectID": "about.html#featured-projects",
    "href": "about.html#featured-projects",
    "title": "About",
    "section": "",
    "text": "Marine Life Classification: Using Convolutional Neural Networks (CNNs) to classify marine species from underwater images.\nOcean Surface Temperature Prediction: A deep learning model to predict temperature patterns across global oceans.\nWhale Detection: Leveraging audio recognition to identify and track whale sounds from oceanic datasets.\nCoral Reef Health Monitoring: Anomaly detection using deep learning to monitor the health of coral reefs through satellite imagery."
  },
  {
    "objectID": "about.html#stay-updated",
    "href": "about.html#stay-updated",
    "title": "About",
    "section": "",
    "text": "You can also follow me on LinkedIn.\n\nThank you for visiting my blog! Dive into the world of ocean scinece AI, and let’s explore the deep blue together 🌊."
  },
  {
    "objectID": "posts/250211-cifar10/04_classification.html",
    "href": "posts/250211-cifar10/04_classification.html",
    "title": "Learning rates and mixed precision training",
    "section": "",
    "text": "Today we’re teaching a neural network to distinguish images from the CIFAR-10 dataset, which contains tiny pictures of airplanes, birds, cats, dogs, and more.\nWe’ll start simple, training a baseline model, and then use it as a jumping off point to play with some new concepts like discriminative learning rates and mixed precision training.\nLet’s roll up our sleeves and dive in!"
  },
  {
    "objectID": "posts/250211-cifar10/04_classification.html#the-best-model",
    "href": "posts/250211-cifar10/04_classification.html#the-best-model",
    "title": "Learning rates and mixed precision training",
    "section": "The Best Model",
    "text": "The Best Model\n\n# Model D\nn_epochs = 32\ncifar = DataBlock(\n    blocks = [ImageBlock, CategoryBlock],\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42),\n    get_y = parent_label,\n    batch_tfms=aug_transforms(size=24, min_scale=0.75)\n)\ndls = cifar.dataloaders(path/'train', \n                        bs = 512, \n                        seed = 42)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlr = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbase_lr = 1.4e-3\nlearn.freeze()\nlearn.fit_one_cycle(3, base_lr)\nlearn.unfreeze()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.458459\n1.714187\n0.432300\n00:23\n\n\n1\n1.839833\n1.476324\n0.483800\n00:25\n\n\n2\n1.624287\n1.423428\n0.499400\n00:26\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\n\n\n\n\n\nbase_lr = 1.4e-3\ndisc_lr = slice(8e-5,8e-3)\nfreeze_epochs = 3\nn_epochs = 32\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.freeze()\nlearn.fit_one_cycle(freeze_epochs, base_lr)\nlearn.unfreeze()\nlearn.fit_one_cycle(n_epochs, disc_lr)\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.492095\n1.698559\n0.433000\n00:29\n\n\n1\n1.857048\n1.463014\n0.495400\n00:30\n\n\n2\n1.633801\n1.423831\n0.501000\n00:28\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.446787\n1.295929\n0.548800\n00:29\n\n\n1\n1.269360\n1.142912\n0.602900\n00:28\n\n\n2\n1.092596\n1.073082\n0.636900\n00:28\n\n\n3\n0.965740\n1.008197\n0.670700\n00:30\n\n\n4\n0.894361\n1.037594\n0.685900\n00:30\n\n\n5\n0.850987\n0.870906\n0.704600\n00:29\n\n\n6\n0.797181\n0.848414\n0.713000\n00:29\n\n\n7\n0.755061\n0.818326\n0.731000\n00:26\n\n\n8\n0.702218\n0.763084\n0.745400\n00:30\n\n\n9\n0.668742\n0.762206\n0.747800\n00:28\n\n\n10\n0.648104\n0.794793\n0.736600\n00:23\n\n\n11\n0.614646\n0.772262\n0.749800\n00:27\n\n\n12\n0.576669\n0.749846\n0.752800\n00:26\n\n\n13\n0.531431\n0.749366\n0.759000\n00:28\n\n\n14\n0.493150\n0.715264\n0.768700\n00:28\n\n\n15\n0.467971\n0.730673\n0.764300\n00:27\n\n\n16\n0.442254\n0.716524\n0.776800\n00:29\n\n\n17\n0.407482\n0.748114\n0.766100\n00:30\n\n\n18\n0.372706\n0.745389\n0.772900\n00:28\n\n\n19\n0.341517\n0.755464\n0.768600\n00:30\n\n\n20\n0.313545\n0.781643\n0.767300\n00:29\n\n\n21\n0.301854\n0.779249\n0.767500\n00:30\n\n\n22\n0.275095\n0.798806\n0.770800\n00:28\n\n\n23\n0.252724\n0.813480\n0.772600\n00:29\n\n\n24\n0.229407\n0.811392\n0.775200\n00:28\n\n\n25\n0.220920\n0.848612\n0.772700\n00:27\n\n\n26\n0.194887\n0.844323\n0.774200\n00:28\n\n\n27\n0.191182\n0.838158\n0.777200\n00:29\n\n\n28\n0.181282\n0.853957\n0.775500\n00:28\n\n\n29\n0.167093\n0.863638\n0.775000\n00:28\n\n\n30\n0.171526\n0.858613\n0.775500\n00:31\n\n\n31\n0.176508\n0.868455\n0.777200\n00:29"
  },
  {
    "objectID": "posts/250211-cifar10/04_classification.html#mixed-precision-training-resnet101",
    "href": "posts/250211-cifar10/04_classification.html#mixed-precision-training-resnet101",
    "title": "Learning rates and mixed precision training",
    "section": "Mixed Precision Training: Resnet101",
    "text": "Mixed Precision Training: Resnet101\nI decided to also test out mixed precision training to allow training of a much larger network, Resnet101, without taking too much time. The accuracy of the model actually did not improve with the deeper network. Seems like it should have, but there’s much more to do before this model is SOTA. ImageNet pretraining may not be as good as simply training from scratch, and best efforts I ran across so far did not use pretrained Imagenet models, but instead started from scratch.\n\nfrom fastai.callback.fp16 import *\n\ncifar = DataBlock(\n    blocks = [ImageBlock, CategoryBlock],\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42),\n    get_y = parent_label,\n    batch_tfms=aug_transforms(size=24, min_scale=0.75)\n)\ndls = cifar.dataloaders(path/'train', \n                        bs = 512, \n                        seed = 42)\nlearn = vision_learner(dls, resnet101, metrics=accuracy).to_fp16()\nlr = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbase_lr = 6e-3\nlearn.freeze()\nlearn.fit_one_cycle(1, base_lr)\nlearn.unfreeze()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.637301\n1.372016\n0.511300\n00:36\n\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00015848931798245758)\n\n\n\n\n\n\n\n\n\n\nbase_lr = 6e-3\ndisc_lr = slice(1e-5,1e-3)\nfreeze_epochs = 1\nn_epochs = 40\n\nlearn = vision_learner(dls, resnet101, metrics=accuracy).to_fp16()\nlearn.freeze()\nlearn.fit_one_cycle(freeze_epochs, base_lr)\nlearn.unfreeze()\nlearn.fit_one_cycle(n_epochs, disc_lr)\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.641111\n1.315193\n0.536400\n00:32\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.329345\n1.266843\n0.555600\n00:40\n\n\n1\n1.246262\n1.180738\n0.581500\n00:41\n\n\n2\n1.160876\n1.114313\n0.608000\n00:41\n\n\n3\n1.059085\n1.024999\n0.646800\n00:40\n\n\n4\n0.967155\n0.956626\n0.678800\n00:41\n\n\n5\n0.883919\n0.877438\n0.696900\n00:41\n\n\n6\n0.804704\n0.896085\n0.718200\n00:40\n\n\n7\n0.770523\n1.013896\n0.705100\n00:41\n\n\n8\n0.723203\n0.837037\n0.723600\n00:44\n\n\n9\n0.664702\n0.789105\n0.733400\n00:44\n\n\n10\n0.621836\n0.814080\n0.736800\n00:41\n\n\n11\n0.582255\n0.809976\n0.730500\n00:41\n\n\n12\n0.536979\n0.932742\n0.732400\n00:41\n\n\n13\n0.503126\n0.868716\n0.740800\n00:40\n\n\n14\n0.470739\n0.868617\n0.744600\n00:40\n\n\n15\n0.438311\n0.787409\n0.751000\n00:42\n\n\n16\n0.406661\n0.824242\n0.752100\n00:40\n\n\n17\n0.370056\n0.818419\n0.755500\n00:40\n\n\n18\n0.331431\n0.803667\n0.755200\n00:42\n\n\n19\n0.304569\n0.832193\n0.755100\n00:41\n\n\n20\n0.278200\n0.821361\n0.763000\n00:41\n\n\n21\n0.258393\n0.866658\n0.756300\n00:43\n\n\n22\n0.236941\n0.860195\n0.761100\n00:44\n\n\n23\n0.221599\n0.907863\n0.755600\n00:41\n\n\n24\n0.211816\n0.896679\n0.757100\n00:42\n\n\n25\n0.184622\n0.904773\n0.759300\n00:41\n\n\n26\n0.180264\n0.901700\n0.764000\n00:41\n\n\n27\n0.151746\n0.918147\n0.767000\n00:41\n\n\n28\n0.135492\n0.911483\n0.773300\n00:42\n\n\n29\n0.131715\n0.927934\n0.762800\n00:41\n\n\n30\n0.122062\n0.953696\n0.761800\n00:41\n\n\n31\n0.120269\n0.952933\n0.765100\n00:42\n\n\n32\n0.103523\n0.952526\n0.765900\n00:41\n\n\n33\n0.097716\n0.958341\n0.768900\n00:40\n\n\n34\n0.095643\n0.974640\n0.766700\n00:40\n\n\n35\n0.094067\n0.964051\n0.769500\n00:43\n\n\n36\n0.088655\n0.946297\n0.771300\n00:41\n\n\n37\n0.085030\n0.962778\n0.769100\n00:42\n\n\n38\n0.082994\n0.951210\n0.770100\n00:41\n\n\n39\n0.085635\n0.962201\n0.767800\n00:41"
  },
  {
    "objectID": "posts/250317-nlp-agnews/08_nlp.html",
    "href": "posts/250317-nlp-agnews/08_nlp.html",
    "title": "ULMFiT on AG News: Fine-Tuning a Wikipedia-Pretrained Model",
    "section": "",
    "text": "In this project, we apply a powerful approach known as Universal Language Model Fine-tuning (ULMFiT). We’ll start from a language model that’s already been trained on large amounts of Wikipedia text, then adapt it (“fine-tune”) to the AG News corpus. After that, we’ll build a text classifier on top of our freshly fine-tuned encoder. The result is a model that can assign news articles into one of four categories—World, Sports, Business, or Sci/Tech."
  },
  {
    "objectID": "posts/250317-nlp-agnews/08_nlp.html#preplanning",
    "href": "posts/250317-nlp-agnews/08_nlp.html#preplanning",
    "title": "ULMFiT on AG News: Fine-Tuning a Wikipedia-Pretrained Model",
    "section": "Preplanning",
    "text": "Preplanning\nBefore we dive in, let’s outline our overall plan:\n\nObjective: Create an NLP model that classifies AG News articles, taking advantage of an existing Wikipedia-pretrained model.\nDataset: Use the AG News corpus, which features four main classes of news stories.\nStrategy:\n\nFine-tune the Wikipedia-trained language model on AG News text. This helps the model learn domain-specific nuances (news vocabulary, style, etc.).\nUse the fine-tuned model as an encoder for classification. Train a classifier head on top of it to predict among four news categories.\n\nTools: We’ll rely on the fastai library’s text APIs, which streamline ULMFiT, letting us focus on the conceptual side.\n\nWith that frame in mind, let’s start coding!\nHere, we download and extract the AG News corpus. The dataset conveniently arrives in CSV format with clearly designated train and test files.\n\nfrom fastai.text.all import *\npath = untar_data(URLs.AG_NEWS)\n\n\npath.ls()\n\n(#4) [Path('/root/.fastai/data/ag_news_csv/test.csv'),Path('/root/.fastai/data/ag_news_csv/readme.txt'),Path('/root/.fastai/data/ag_news_csv/classes.txt'),Path('/root/.fastai/data/ag_news_csv/train.csv')]\n\n\n\n!cat /root/.fastai/data/ag_news_csv/readme.txt\n\nAG's News Topic Classification Dataset\n\nVersion 3, Updated 09/09/2015\n\n\nORIGIN\n\nAG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity. For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n\nThe AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n\n\nDESCRIPTION\n\nThe AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n\nThe file classes.txt contains a list of classes corresponding to each label.\n\nThe files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 to 4), title and description. The title and description are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\n\n\n\n!cat /root/.fastai/data/ag_news_csv/classes.txt\n\nWorld\nSports\nBusiness\nSci/Tech"
  },
  {
    "objectID": "posts/250317-nlp-agnews/08_nlp.html#inspect-the-dataset-and-get-the-class-labels",
    "href": "posts/250317-nlp-agnews/08_nlp.html#inspect-the-dataset-and-get-the-class-labels",
    "title": "ULMFiT on AG News: Fine-Tuning a Wikipedia-Pretrained Model",
    "section": "Inspect the Dataset and Get the Class Labels",
    "text": "Inspect the Dataset and Get the Class Labels\nBecause the data is in CSV files, we’ll read them using pandas. We combine train/test to create a “grand corpus” that we can use to fine-tune the language model. The variable df_lm now contains the text from both splits— perfect for language modeling.\n\ncol_names = [\"class_index\", \"title\", \"description\"]\ndf_train = pd.read_csv(path/'train.csv', header=None, names=col_names, low_memory=False)\ndf_valid = pd.read_csv(path/'test.csv', header=None, names=col_names, low_memory=False)\ndf_lm = pd.concat([df_train, df_valid], axis=0, ignore_index=True)\n\n\ndls_lm = TextDataLoaders.from_df(df_lm, path=path, text_col=(1,2), label_col=0, is_lm=True, seed=42, shuffle=False)\n\n\n\n\n\n\n\n\n\ndls_lm.train.show_batch()\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\nxxbos xxfld 1 xxup fg xxmaj holds xxmaj talks with xxmaj niger xxmaj delta xxmaj militants xxfld 2 xxmaj the xxmaj federal xxmaj government yesterday revealed that security agencies are holding talks in xxmaj abuja with the leadership of the xxmaj niger xxmaj delta xxmaj peoples xxmaj volunteer xxmaj force ( xxunk ) , led by xxmaj xxunk xxmaj asari xxmaj dokubo , over the continued unrest in the xxmaj niger xxmaj\nxxfld 1 xxup fg xxmaj holds xxmaj talks with xxmaj niger xxmaj delta xxmaj militants xxfld 2 xxmaj the xxmaj federal xxmaj government yesterday revealed that security agencies are holding talks in xxmaj abuja with the leadership of the xxmaj niger xxmaj delta xxmaj peoples xxmaj volunteer xxmaj force ( xxunk ) , led by xxmaj xxunk xxmaj asari xxmaj dokubo , over the continued unrest in the xxmaj niger xxmaj delta\n\n\n1\nxxmaj street xxmaj global xxmaj advisors , xxmaj timothy xxup b. xxmaj xxunk , died of a heart attack on xxmaj tuesday night , the company said yesterday . xxmaj he was 53 . xxbos xxfld 1 xxmaj texas not popular choice xxfld 2 xxmaj the xxmaj breeders ' xxmaj cup is making its first visit to xxmaj texas and not everyone thinks the world of it . xxmaj there has been\nstreet xxmaj global xxmaj advisors , xxmaj timothy xxup b. xxmaj xxunk , died of a heart attack on xxmaj tuesday night , the company said yesterday . xxmaj he was 53 . xxbos xxfld 1 xxmaj texas not popular choice xxfld 2 xxmaj the xxmaj breeders ' xxmaj cup is making its first visit to xxmaj texas and not everyone thinks the world of it . xxmaj there has been grumbling\n\n\n2\nthe body to grow its own bypasses , at first in the legs and , if that works , perhaps later in the heart … xxbos xxfld 1 xxup eu : xxmaj coke xxup eu anti - trust case settlement closer xxfld 2 coca - cola xxmaj xxunk xxmaj bottling xxmaj company has announced developments in the long - running xxup eu investigation into anti - competitive practices . xxbos xxfld 1\nbody to grow its own bypasses , at first in the legs and , if that works , perhaps later in the heart … xxbos xxfld 1 xxup eu : xxmaj coke xxup eu anti - trust case settlement closer xxfld 2 coca - cola xxmaj xxunk xxmaj bottling xxmaj company has announced developments in the long - running xxup eu investigation into anti - competitive practices . xxbos xxfld 1 xxmaj\n\n\n3\nbelieves xxmaj dennis xxmaj rommedahl 's sparkling winner will provide the platform for the xxmaj dane to recapture the form that made him one of xxmaj europe 's most feared wingers . xxbos xxfld 1 xxmaj business as usual for xxup ata despite bankruptcy filing xxfld 2 xxmaj although the future of xxup ata xxmaj airlines xxmaj inc . remains up in the air , planes still were landing xxmaj wednesday at\nxxmaj dennis xxmaj rommedahl 's sparkling winner will provide the platform for the xxmaj dane to recapture the form that made him one of xxmaj europe 's most feared wingers . xxbos xxfld 1 xxmaj business as usual for xxup ata despite bankruptcy filing xxfld 2 xxmaj although the future of xxup ata xxmaj airlines xxmaj inc . remains up in the air , planes still were landing xxmaj wednesday at the\n\n\n4\nof xxunk percent . xxmaj it is , state officials said , the lowest interest rate offered in more than 30 years . xxbos xxfld 1 xxup n.c . xxmaj state 's xxmaj hodge xxmaj puts xxmaj self xxmaj among xxmaj elite ( ap ) xxfld 2 xxup ap - xxmaj julius xxmaj hodge drove to the basket and scored , then turned to run back on defense . xxmaj on the\nxxunk percent . xxmaj it is , state officials said , the lowest interest rate offered in more than 30 years . xxbos xxfld 1 xxup n.c . xxmaj state 's xxmaj hodge xxmaj puts xxmaj self xxmaj among xxmaj elite ( ap ) xxfld 2 xxup ap - xxmaj julius xxmaj hodge drove to the basket and scored , then turned to run back on defense . xxmaj on the way\n\n\n5\ntuesday xxbos xxfld 1 xxmaj bryant 's xxmaj accuser xxmaj must xxmaj be xxmaj identified xxfld 2 xxup denver - a federal judge in xxmaj colorado has rejected a request from the woman accusing xxup nba star xxmaj kobe xxmaj bryant of rape to remain anonymous in her civil lawsuit . xxbos xxfld 1 xxmaj sox and bonds xxfld 2 xxmaj there are games in which players renew their vows as teammates\nxxbos xxfld 1 xxmaj bryant 's xxmaj accuser xxmaj must xxmaj be xxmaj identified xxfld 2 xxup denver - a federal judge in xxmaj colorado has rejected a request from the woman accusing xxup nba star xxmaj kobe xxmaj bryant of rape to remain anonymous in her civil lawsuit . xxbos xxfld 1 xxmaj sox and bonds xxfld 2 xxmaj there are games in which players renew their vows as teammates ,\n\n\n6\nxxmaj boston to the 6 - 2 win in xxmaj game 2 . xxbos xxfld 1 xxup us xxmaj consumer xxmaj confidence xxmaj tumbles in xxmaj august xxfld 2 xxup new xxup york ( reuters ) - xxup u.s . consumer confidence fell sharply in xxmaj august , breaking four straight months of gains , as a slowdown in job creation and rising oil prices weighed on sentiment . xxbos xxfld 1\nboston to the 6 - 2 win in xxmaj game 2 . xxbos xxfld 1 xxup us xxmaj consumer xxmaj confidence xxmaj tumbles in xxmaj august xxfld 2 xxup new xxup york ( reuters ) - xxup u.s . consumer confidence fell sharply in xxmaj august , breaking four straight months of gains , as a slowdown in job creation and rising oil prices weighed on sentiment . xxbos xxfld 1 temple\n\n\n7\n\\ \\ xxmaj early xxmaj internet surfers , which of course means something less than a decade ago for most of us , will remember the days when xxmaj netscape was the only real browser in town . xxmaj for those who do n't remember , there was a time when xxmaj netscape was for sale in … xxbos xxfld 1 xxmaj everton ' considering ' increased xxmaj rooney bid xxfld 2\n\\ xxmaj early xxmaj internet surfers , which of course means something less than a decade ago for most of us , will remember the days when xxmaj netscape was the only real browser in town . xxmaj for those who do n't remember , there was a time when xxmaj netscape was for sale in … xxbos xxfld 1 xxmaj everton ' considering ' increased xxmaj rooney bid xxfld 2 xxmaj\n\n\n8\nreuters ) xxfld 2 xxmaj reuters - xxmaj the xxmaj japanese government downgraded its \\ view on the economy slightly on xxmaj tuesday , citing weaker exports \\ and output , but it said a recovery was continuing due to steady \\ domestic demand , in terms of both personal consumption and \\ capital spending . \" the economy continues to recover , while \\ some weak movements have been seen recently\n) xxfld 2 xxmaj reuters - xxmaj the xxmaj japanese government downgraded its \\ view on the economy slightly on xxmaj tuesday , citing weaker exports \\ and output , but it said a recovery was continuing due to steady \\ domestic demand , in terms of both personal consumption and \\ capital spending . \" the economy continues to recover , while \\ some weak movements have been seen recently ,\n\n\n\n\n\nHere we are grouping the title and description columns for the model’s text input in a single feature. Note that the independent variable text and the dependent target text_ are offset by a single token. Our fine-tuned language model will learn to predict the next token in the series based on the values in the stream text."
  },
  {
    "objectID": "posts/250317-nlp-agnews/08_nlp.html#fine-tune-the-language-model",
    "href": "posts/250317-nlp-agnews/08_nlp.html#fine-tune-the-language-model",
    "title": "ULMFiT on AG News: Fine-Tuning a Wikipedia-Pretrained Model",
    "section": "Fine-Tune the Language Model",
    "text": "Fine-Tune the Language Model\nWe’ll base training on the classic AWD-LSTM model (as used in ULMFiT), initialize and train it. Note that we’ll do a quick pass first, then unfreeze more layers for deeper fine-tuning. Because language model is so large, we’ll also start off training in mixed precision to save time and resources.\n\nlearn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3,\n    metrics=[accuracy, Perplexity()]).to_fp16()\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\n\n\n    \n      \n      100.00% [105070592/105067061 00:01&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n3.208871\n3.068084\n0.448090\n21.500673\n10:58\n\n\n\n\n\n\nlearn.save('1epoch')\nlearn = learn.load('1epoch')\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3)\nlearn.save_encoder('finetuned')\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n2.986610\n2.879777\n0.471676\n17.810308\n11:50\n\n\n1\n2.830561\n2.799445\n0.482623\n16.435516\n11:54\n\n\n2\n2.637185\n2.740306\n0.493215\n15.491731\n11:57\n\n\n3\n2.456956\n2.715974\n0.499630\n15.119334\n11:57\n\n\n4\n2.294566\n2.704736\n0.505257\n14.950377\n11:56\n\n\n5\n2.147095\n2.698480\n0.510063\n14.857126\n11:58\n\n\n6\n2.005627\n2.695754\n0.514727\n14.816684\n11:58\n\n\n7\n1.902904\n2.703961\n0.518032\n14.938793\n12:00\n\n\n8\n1.820011\n2.705783\n0.520096\n14.966028\n12:01\n\n\n9\n1.797798\n2.743584\n0.518972\n15.542585\n12:02\n\n\n\n\n\nAn accuracy of 52% might not sound high, but it’s actually a really good value for a language model predicting the next word in a sentence. Common values for this on other datasets are also in the 40-50% range. Because the model took hours to train, we should save it now.\n\nlearn.save_encoder('finetuned')\n\nSome important components to note from the code above:\n\nlanguage_model_learner: Loads a pretrained LM (from Wikipedia), plus a final layer we’ll adapt to our new text domain.\nfit_one_cycle: A training schedule that typically yields fast, stable convergence.\nunfreeze(): Unlocks earlier layers, allowing the model to adjust them for our AG News text.\nsave_encoder(‘finetuned’): Saves only the encoder portion of the model. We’ll need this for classification."
  },
  {
    "objectID": "posts/250317-nlp-agnews/08_nlp.html#sanity-check-generate-some-text",
    "href": "posts/250317-nlp-agnews/08_nlp.html#sanity-check-generate-some-text",
    "title": "ULMFiT on AG News: Fine-Tuning a Wikipedia-Pretrained Model",
    "section": "Sanity Check: Generate Some Text",
    "text": "Sanity Check: Generate Some Text\nLanguage models can have fun creative uses: we can sample from them to see if the fine-tuning “vocabulary” looks correct.\n\nTEXT = \"Netflix Launches New\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n         for _ in range(N_SENTENCES)] \nprint(\"\\n\".join(preds))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetflix Launches New DVD Rental Service ( ap ) xxfld 2 AP - The DVD rental business is offering a boost to a new DVD rental service , the Blu - ray Disc ,\nNetflix Launches New Online Service ( ap ) xxfld 2 AP - Online video service Netflix Inc . on Monday said it was launching a service that lets users watch movies , music and games faster\n\n\nIf the output reads like plausible news text about Netflix, we know the domain adaptation is at least somewhat working."
  },
  {
    "objectID": "posts/250317-nlp-agnews/08_nlp.html#discriminative-learning-rates-and-gradual-unfreezing",
    "href": "posts/250317-nlp-agnews/08_nlp.html#discriminative-learning-rates-and-gradual-unfreezing",
    "title": "ULMFiT on AG News: Fine-Tuning a Wikipedia-Pretrained Model",
    "section": "Discriminative Learning Rates and Gradual Unfreezing",
    "text": "Discriminative Learning Rates and Gradual Unfreezing\nA hallmark of ULMFiT is the notion that earlier layers need less aggressive updates than later ones. So we train in stages, unfreezing layer by layer and applying smaller LR to earlier layers.\n\nlearn.fit_one_cycle(1, 2e-2)\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n0.728121\n0.365699\n0.879958\n1.441521\n01:41\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n0.735102\n0.360706\n0.898500\n1.434342\n01:55\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n0.777047\n0.286184\n0.919708\n1.331338\n02:50\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n0.480853\n0.317520\n0.887042\n1.373717\n03:42\n\n\n1\n0.777151\n0.304685\n0.920875\n1.356198\n03:42\n\n\n\n\n\nEach training session here expands the set of trainable layers until everything is fine-tuned together.\nWe can see how well the classifier is assigning categories by examining sample predictions.\n\nlearn.show_results(max_n=6)\n\n\n\n\n\n\n\n\n\n\n\n\ntext\ncategory\ncategory_\n\n\n\n\n0\nxxbos xxfld 1 xxmaj area xxmaj college xxmaj football xxmaj capsules xxfld 2 xxmaj navy at xxmaj tulsa &lt; br &gt; xxmaj where : xxmaj xxunk xxmaj stadium xxmaj when : 7 p.m. &lt; br &gt; xxmaj shooting for 3 - 0 : xxmaj navy is off to its first 2 - 0 start since 1996 . xxmaj the xxmaj midshipmen have n't started 3 - 0 since 1979 , when they won their first six games and finished 7 - 4 . xxmaj navy has started 3 - 0 only twice in the past 40 years -- the 1978 team won its first seven games . xxmaj tulsa , which improved from 1 - 11 in 2002 to 8 - 5 last season , the best turnaround in college football , has lost its first two games , 21 - 3 at xxmaj kansas and 38 - 21 at\n2\n2\n\n\n1\nxxbos xxfld 1 xxmaj munch xxmaj theft xxmaj focuses on xxmaj museum xxmaj security xxfld 2 xxup oslo , xxmaj norway - xxmaj the brazen daylight theft of xxmaj edvard xxmaj munch 's renowned masterpiece \" the xxmaj scream \" left xxmaj norway 's police scrambling for clues and stirred a debate across xxmaj europe over how to protect art if thieves are willing to use deadly force to take it . xxmaj some expressed fears that works of art are in increasing danger from violent raids - unless , as xxmaj norway 's deputy culture minister put it , \" we lock them in a mountain bunker . \" xxmaj armed , masked robbers stormed into xxmaj oslo 's xxmaj munch xxmaj museum in broad daylight on xxmaj sunday , threatening an employee with a gun and terrifying patrons before they made off with a version of xxmaj munch\n1\n1\n\n\n2\nxxbos xxfld 1 xxup xxunk xxmaj lands xxup faa xxmaj conversations xxfld 2 \\ \" in response to its xxup xxunk request , xxup epic has received from the xxmaj federal xxmaj aviation \\ xxmaj administration ( faa ) transcripts ( pdf ) and audio recordings concerning the \\ request by the office of xxup us xxmaj house of xxmaj representatives xxmaj majority xxmaj leader xxmaj tom delay \\ ( r - tx ) to the xxup faa regarding the xxmaj may 2003 search for the plane owned by xxmaj texas \\ xxmaj state xxmaj representative xxmaj pete xxmaj xxunk ( tail xxmaj number xxup xxunk ) . \" \\ \" the xxmaj may 12 , 2003 audio recording of telephone conversations between the faa 's \\ xxmaj washington xxmaj operations xxmaj center and various xxup faa field employees clearly indicate \\ that the xxup faa employees were misled into\n4\n3\n\n\n3\nxxbos xxfld 1 mysql and xxup alter xxup table xxmaj guilty as xxmaj charged xxfld 2 \\ \\ xxmaj for the last few days xxmaj i 've been using xxunk xxup alter and xxup repair table \\ functionality and its caused tons of countless problems and a great deal of lost \\ sleep . \\ \\ xxmaj the first problem i noticed was that for large tables xxup alter xxup table was taking \\ hours ! xxmaj lets say you have a 30 g table . xxmaj good luck altering it as the default \\ mysql configuration will probably take 100 or more hours . \\ \\ xxmaj in xxunk defense there are a number of variables you can use to increase the \\ performance of an xxup alter but the problem is that the two major ones \\ ( xxunk , and xxunk ) ca n't be set at\n4\n4\n\n\n4\nxxbos xxfld 1 xxmaj tressel xxmaj trailed by xxmaj allegations xxfld 2 xxmaj oh , if only the biggest problems in xxmaj columbus , xxmaj ohio , were how the xxmaj buckeyes might get their running game going and beat xxmaj purdue today . xxmaj not so . xxmaj in a pair of stories -- one in xxup espn the xxmaj magazine , the other on espn.com -- xxmaj ohio xxmaj state xxmaj coach xxmaj jim xxmaj tressel was first accused by former star running back xxmaj maurice xxmaj clarett of helping him gain access to free cars and of hooking him up with boosters for cash payments . xxmaj the second story traced such scams back to xxmaj tressel 's days as the coach at xxmaj youngstown xxmaj state , in xxmaj clarett 's home town . xxmaj ohio xxmaj state 's response to xxmaj clarett : xxmaj he\n2\n2\n\n\n5\nxxbos xxfld 1 xxmaj skype dials up beta software for xxmaj mac xxup os x xxfld 2 xxmaj skype xxmaj technologies xxup sa , of xxmaj luxembourg , xxmaj tuesday released a beta version of its free xxmaj internet telephony software for xxmaj apple xxmaj computer xxmaj inc . 's xxmaj mac xxup os xxunk &gt; advertisement &lt; / p&gt;&lt;p&gt;&lt;img src=\"http : / / ad.doubleclick.net / ad / idg.us.ifw.general / ibmpseries;sz=1x1;ord=200301151450 ? \" width=\"1 \" height=\"1 \" border=\"0 \" / &gt; &lt; a href=\"http : / / ad.doubleclick.net / clk;9824455;9690404;u?http : / / ad.doubleclick.net / clk;9473681;9688522;d?http : / / xxrep 3 w .ibm.com / servers / eserver / pseries / campaigns / boardroom / index.html?ca=pseries met = boardroom me = e p_creative = p_infow_rss\"&gt;introducing xxup ibm eserver p5 systems . &lt; / a&gt;&lt;br / &gt; powered by ibms most advanced 64 - bit microprocessor ( power5(tm ) ) , p5\n4\n4"
  },
  {
    "objectID": "posts/250317-nlp-agnews/08_nlp.html#pushing-accuracy-further",
    "href": "posts/250317-nlp-agnews/08_nlp.html#pushing-accuracy-further",
    "title": "ULMFiT on AG News: Fine-Tuning a Wikipedia-Pretrained Model",
    "section": "Pushing Accuracy Further",
    "text": "Pushing Accuracy Further\nFinally, we do a longer run of training to refine the classifier further:\n\nlearn.fit_one_cycle(12, slice(1e-3/(2.6**4),1e-3))\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n0.519692\n0.277178\n0.916292\n1.319401\n03:42\n\n\n1\n0.410583\n0.252542\n0.920333\n1.287294\n03:42\n\n\n2\n0.382233\n0.247313\n0.924000\n1.280580\n03:41\n\n\n3\n0.346298\n0.247176\n0.924750\n1.280404\n03:42\n\n\n4\n0.317382\n0.251329\n0.926708\n1.285733\n03:42\n\n\n5\n0.293783\n0.269677\n0.925250\n1.309541\n03:42\n\n\n6\n0.278939\n0.263954\n0.929875\n1.302068\n03:42\n\n\n7\n0.266410\n0.263146\n0.931000\n1.301017\n03:42\n\n\n8\n0.259386\n0.274693\n0.932625\n1.316127\n03:41\n\n\n9\n0.272189\n0.277544\n0.935833\n1.319884\n03:42\n\n\n10\n0.295776\n0.298814\n0.933667\n1.348259\n03:42\n\n\n11\n0.274800\n0.292780\n0.934333\n1.340148\n03:42\n\n\n\n\n\nThis is typically where you might pick up a few extra percentage points of accuracy.\nIn order to achieve State of the Art Results, we could continue onward now to also do a forward-backward ensemble approach. This involves training the model on reversed text, and making predictions based on an ensemble of the forward-trained model with the backward-trained model. For now we’ll leave that to a future blog post."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html",
    "href": "posts/250203-fish-types/03_fish_types.html",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "",
    "text": "Today, we’re going to build something fast and easy, and check out a way to quickly compile and clean a dataset gathered from the Bing Image Search API. This fish classifier is built using Pytorch and FastAI, and reaches 93% accuracy using a very small dataset, only 380 images in total. This classifier can differentiate between fish that belong on coral reefs, freshwater, or in the deep ocean. Let’s dive in!"
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#pre-planning",
    "href": "posts/250203-fish-types/03_fish_types.html#pre-planning",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Pre-planning",
    "text": "Pre-planning\nBefore we dive straight in to model building, let’s think like an engineer. What do we actually want to accomplish?\n\nDefine the objective – We want to build a model that can classify images into deep sea, reef, or freshwater fish. Not just the ones in our dataset, but any fish picture we throw at it.\nWhat actions can we take? – We can gather a dataset of fish images, clean it up so it’s not full of junk, and train a model to be as accurate as possible.\nWhat data do we have? – The internet is full of fish pictures! We’ll use Bing’s image search API to scrape some and build our own dataset."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#gathering-a-dataset",
    "href": "posts/250203-fish-types/03_fish_types.html#gathering-a-dataset",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Gathering a Dataset",
    "text": "Gathering a Dataset\nFirst, let’s grab some fish pictures from Bing.\n\nfrom fastai.vision.all import *\n\n\nkey = os.environ.get('AZURE_SEARCH_KEY', 'my_api_key')  # insert key value here\npath = Path(\"fish\")\n\nSEARCH_TERMS = [\"deep sea fish\", \"freshwater fish\", \"reef fish\"]\nfor o in SEARCH_TERMS:\n    dest = path/o\n    if not os.path.exists(dest):\n        os.makedirs(dest)\n        results = search_images_bing(key, o)\n        download_images(dest, urls=results.attrgot('contentUrl')) # dest is a path object \n\nGreat, now we have folders full of images. But the internet is messy— some of these are probably not fish, some might be mislabeled, and some might be totally useless. We need to clean up."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#cleaning-the-dataset",
    "href": "posts/250203-fish-types/03_fish_types.html#cleaning-the-dataset",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Cleaning the dataset",
    "text": "Cleaning the dataset\nInstead of going through them by hand (boring! slow!), we’ll train a quick classifier to help us sort out the bad ones. First we can remove any obviously broken files.\n\nfns = get_image_files(path)  # finds all image files in path and subpaths\nfailed = verify_images(fns)\nfailed.map(Path.unlink)\n\nThen train a quick model to help us clean the rest.\n\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(224, min_scale=0.5),\n                                      batch_tfms=aug_transforms())\n\n\ndls.valid.show_batch(max_n=5, nrows=1)\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\n\n\n\n\n\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.648003\n0.747909\n0.671053\n00:18\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.796192\n0.540283\n0.776316\n00:16\n\n\n1\n0.682664\n0.470164\n0.815789\n00:16\n\n\n2\n0.550159\n0.427897\n0.828947\n00:16\n\n\n\n\n\nLet’s use this model to find the images it’s most confused about. Those images are likely misclassified or just bad images.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(10, nrows=4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom fastai.vision.widgets import *\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis pulls up an interactive widget where we can delete the bad images or move them to the correct category.\n\n\n\nImageClassifierCleaner\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nBoom! Dataset cleaned. Now we can train the real model."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#experimenting-with-the-model",
    "href": "posts/250203-fish-types/03_fish_types.html#experimenting-with-the-model",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Experimenting with the model",
    "text": "Experimenting with the model\nNow that we have a solid dataset, we can first build a baseline model and then experiment with hyperparameters.\n\nBaseline Model\n\n### Experiment 1: Baseline\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(224, min_scale=0.5),\n                                      batch_tfms=aug_transforms())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.110603\n0.928864\n0.636364\n00:14\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.876281\n0.530159\n0.779221\n00:14\n\n\n1\n0.707189\n0.526176\n0.831169\n00:12\n\n\n2\n0.625902\n0.444526\n0.857143\n00:12\n\n\n\n\n\nThis gives us a good starting point, but we can do better!\n\n\nLarger Image Size\nMaybe the model just needs to see more details in the fish.\n\n### Experiment 2: Larger images\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(500, min_scale=0.5),\n                                      batch_tfms=aug_transforms())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.968835\n0.604990\n0.792208\n00:18\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.743671\n0.430460\n0.818182\n00:19\n\n\n1\n0.598641\n0.391256\n0.857143\n00:18\n\n\n2\n0.471975\n0.371211\n0.857143\n00:18\n\n\n\n\n\nContinuing on, we can play other hyperparameters like with min_scale value, try some deeper or different model architectures, and finally train to overfitting to discover the best number of epochs to train for. Once we’re happy, we save our model for future use."
  },
  {
    "objectID": "posts/250203-fish-types/03_fish_types.html#save-the-final-model",
    "href": "posts/250203-fish-types/03_fish_types.html#save-the-final-model",
    "title": "Classifying deep sea, reef, and freshwater fishes with a simple classifier",
    "section": "Save the final model",
    "text": "Save the final model\n\n### Final Model\ndls = ImageDataLoaders.from_path_func(path, \n                                      get_image_files(path), \n                                      parent_label, \n                                      seed=42,\n                                      item_tfms=RandomResizedCrop(500, min_scale=0.75),\n                                      batch_tfms=aug_transforms())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(5)\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.615644\n0.802557\n0.644737\n00:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.800859\n0.560063\n0.736842\n00:21\n\n\n1\n0.667470\n0.369439\n0.828947\n00:21\n\n\n2\n0.520393\n0.288026\n0.907895\n00:22\n\n\n3\n0.423325\n0.282853\n0.921053\n00:20\n\n\n4\n0.363907\n0.296785\n0.934211\n00:20\n\n\n\n\n\n\nlearn.export()\n\nWe cleaned the dataset and gained significantly in accuracy, hitting 93% on a model trained on only 380 images. If we choose, we can gain rapidly in accuracy by spending a bit more time adding to and continuing to clean the dataset."
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "",
    "text": "Today, I want to show you how to take on the classic CIFAR-10 image classification problem and outmaneuver the top-published ResNet-18 models on the Papers with Code Leaderboard. CIFAR-10 is a widely beloved dataset of 32×32 color images across ten classes (cats, dogs, frogs, ships, airplanes, etc.). Because of its modest size, it’s a perfect playground to mix and match advanced tricks—and see how far we can push a standard ResNet-18.\nLet’s roll up our sleeves and not be afraid to experiment. We’ll do a bit of “what if” poking around until we find the best approach. By the end, you’ll see how easy it is to break new ground with the right techniques."
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html#inspect-the-dataset",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html#inspect-the-dataset",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "Inspect the dataset",
    "text": "Inspect the dataset\nHere, we’ll look at the images, do some basic transformations (random augmentations, normalization, resizing), and then check that our data is normalized around zero mean and unit standard deviation. This normalizing step is typically crucial to help models train consistently.\n\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_20pct_files,\n                   get_y=parent_label,\n                   splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                   batch_tfms=[*aug_transforms(size=24, min_scale=0.75),\n                               Normalize.from_stats(*cifar_stats)])\n\n\ndls = dblock.dataloaders(path/'train', bs=512, seed=42)\n\nWe then do a quick mean/std check: if the mean is close to zero and std is close to one, we know our normalization is in good shape.\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n(TensorImage([ 0.0049, -0.0181, -0.0277], device='cuda:0'),\n TensorImage([0.9488, 0.9568, 0.9518], device='cuda:0'))"
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html#helper-functions-for-dataloaders",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html#helper-functions-for-dataloaders",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "Helper Functions for DataLoaders",
    "text": "Helper Functions for DataLoaders\nWe’ll define a few variants of get_dls(…) so we can try different transformations quickly—sometimes with normalization, sometimes without, sometimes with more or fewer augmentations.\n\n\nCode\ndef get_dls(bs, size, aug_size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_20pct_files,\n                   get_y=parent_label,\n                   splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                   item_tfms=Resize(size),\n                   batch_tfms=[*aug_transforms(size=aug_size, min_scale=0.75),\n                               Normalize.from_stats(*cifar_stats)])\n    return dblock.dataloaders(path/'train', bs=bs, seed=42)\n\ndef get_dls_no_norm(bs, size, aug_size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       get_items=get_20pct_files,\n                       get_y=parent_label,\n                       splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                       item_tfms=Resize(size),\n                       batch_tfms=aug_transforms(size=aug_size, min_scale=0.75))\n    return dblock.dataloaders(path/'train', bs=bs, seed=42)\n\ndef get_dls_no_norm_no_aug(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       get_items=get_20pct_files,\n                       get_y=parent_label,\n                       splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                       item_tfms=Resize(size))\n    return dblock.dataloaders(path/'train', bs=bs, seed=42)\n\ndef get_dls_no_aug(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       get_items=get_20pct_files,\n                       get_y=parent_label,\n                       splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                       item_tfms=Resize(size),\n                       batch_tfms=Normalize.from_stats(*cifar_stats))\n    return dblock.dataloaders(path/'train', bs=bs, seed=42)\n\n\nLooking at a batch of CIFAR-10 data, you see it’s 32x32 pixel color images. Many of these are hard for me to pick a label for, so a 96% success rate by the model might even outperform my human capability.\n\ndls.valid.show_batch(max_n=12, nrows=3, ncols=4)\n\n\n\n\n\n\n\n\n\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n\n(torch.Size([512, 3, 24, 24]), torch.Size([512]))"
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html#baseline-model",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html#baseline-model",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "Baseline Model",
    "text": "Baseline Model\n47.8% Accuracy\nHere’s our baseline training—no fancy bells or whistles, just to see where we stand. I’ll show the code; normally we’d let it run and check the final accuracy. a\n\n# Baseline Model\nn_epochs=32\nlearn.fit(n_epochs)\n\nThe baseline might hover around 47–50% accuracy. That’s a simple reference point—like measuring the “un-augmented” skill of the model. We’ll now run a bunch of experiments, each time reloading the data with different transforms or training hyperparameters, and noting the final accuracy. (I’ve trimmed the epoch-by-epoch logs for brevity.)"
  },
  {
    "objectID": "posts/250213-cifar10-sota/05_sota_classification.html#model-experiments",
    "href": "posts/250213-cifar10-sota/05_sota_classification.html#model-experiments",
    "title": "Surpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10",
    "section": "Model Experiments",
    "text": "Model Experiments\nLeader: Model D2\nAccuracy: 93.6%\n\nModel Z - Original size, resnet18\nAccuracy ~ 65.6%. Just by using original 32×32 images with minimal augmentation, we get about two-thirds accuracy.\n\n# Model Z\ndls = get_dls(512, 32, 24)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(10, base_lr=2e-2, freeze_epochs=3)\n\n\n\nModel Z2 - Original size, smaller bs, resnet18\nAccuracy ~ 70.9%. Smaller batch sizes sometimes help or hurt, depending on GPU memory or other hyperparameter interactions. We get a boost here.\n\n# Model Z2\ndls = get_dls(64, 32, 24)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(10, base_lr=2e-2, freeze_epochs=3)\n\n\n\nModel A - Size 128, resnet18, small bs\nWe scale images to 128×128, do a bit more augmentation, and see what happens:\nAccuracy ~ 91.3%. A big jump—resizing those tiny CIFAR-10 images to 128×128 and applying more data augmentation helps the model see more varied data.\n\n## Model A\ndls = get_dls(64, 128, 96)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlr = learn.lr_find()\n\n\nlr = 3e-3\nn_epochs = 32\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel A2 - Size 128, resnet18, small bs, more freeze_epochs\nAccuracy ~ 90.9%. Freezing the early layers a bit longer sometimes changes the final outcome. In this run, it’s slightly less than 91.3%.\n\n# Model A2\ndls = get_dls(64, 128, 96)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlr = 3e-3\nn_epochs = 32\nlearn.fine_tune(n_epochs, base_lr=lr, freeze_epochs=3)\n\n\n\nModel B- Size 128, resnet18, small bs, no norm\nNo normalization to see how important standardizing the data might be. Accuracy ~ 90.2%. Still respectable, but generally standardization is beneficial.\n\n# Model B\ndls = get_dls_no_norm(64, 128, 96)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\n#learn.lr_find()\n\n\nlr = 3e-3\nn_epochs = 32\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel C- Size 128, resnet18, small bs, no aug\nAccuracy ~ 91.0%. So even with no random augmentations (but still normalized), we break 90%.\n\n# Model C\ndls = get_dls_no_aug(64, 128)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.lr_find()\n\n\nlr = 3e-3\nn_epochs = 32\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel C3- Size 256, resnet18, small bs\nLet’s go bigger still—256×256. With bigger images, the model sees more details: Accuracy: 92.0%\n\n# Model C3\ndls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlr = learn.lr_find()\n\n\nlr = 3e-3\nn_epochs = 32\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel D - Mixup and Label Smoothing\nAccuracy ~ 93.5%. This is a hefty leap.\nTwo powerful techniques:\nMixUp: randomly blends two images and their labels, forcing the model not to rely on too-literal image cues. Label Smoothing: slightly softens the one-hot labels, mitigating overconfidence.\n\n# Model D - Mixup and Label Smoothing\ndls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.lr_find()\n\n\nlr=3e-3\nn_epochs=32\nlearn = vision_learner(dls, resnet18, loss_func=LabelSmoothingCrossEntropy(),\n                metrics=accuracy, cbs=MixUp())\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel E0 - Progressive Resizing\nAccuracy: 90.0%\nWe train first on smaller images, then gradually increase their size. This can help if you’re using a pretrained model so it first learns broad features, then refines them with bigger images. But on CIFAR-10, which is already small, it’s not always beneficial.\n\n# Model E0 - Progressive Resizing\nn_epochs = 16\nlr = 3e-3\n\ndls = get_dls(64, 128, 96)\nlearn = vision_learner(dls, resnet18, pretrained=False, loss_func=LabelSmoothingCrossEntropy(),\n                metrics=accuracy, cbs=MixUp())\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\nlearn.dls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(n_epochs, base_lr=lr)\n\n\n\nModel F- Discriminative Learning Rates\nAccuracy: 90.8%\nThe idea: later layers can use higher learning rates while earlier, more general layers use a lower one. In fastai, we do this with a range (slice(lr/10, lr*10)).\n\n# Model F\nlr=3e-3\nn_epochs=32\ndls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, loss_func=LabelSmoothingCrossEntropy(),\n                metrics=accuracy, cbs=MixUp())\nlearn.fit_one_cycle(1, lr_max=lr)\nlearn.lr_find()\n\n\nlr=2e-3\nlearn.fit_one_cycle(n_epochs, lr_max=slice(lr/10,lr*10))\n\n\n\nModel D2- Mixup and Label Smoothing, Mixed Point Precision\nAccuracy: 93.6%\nWe add one more trick: Mixed-precision training (a.k.a. fp16). It runs your forward/backward passes in half precision on the GPU, speeding things up and sometimes improving generalization.\n\n# Model D Mixed Point\ndls = get_dls(64, 256, 196)\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.lr_find()\nlr=3e-3\nn_epochs=32\nlearn = vision_learner(dls, resnet18, loss_func=LabelSmoothingCrossEntropy(),\n                metrics=accuracy, cbs=MixUp()).to_fp16()\nlearn.fine_tune(n_epochs, base_lr=lr)"
  },
  {
    "objectID": "posts/250317-adults-tabular-sota/07_tabular.html",
    "href": "posts/250317-adults-tabular-sota/07_tabular.html",
    "title": "A State-of-the-Art Random Forest: Beating the Leaderboard on Adult Census Data",
    "section": "",
    "text": "In this post, I’ll walk you through a fun, iterative journey toward training a top-performing model on the famous Adult dataset. This dataset, derived from the US Census Bureau, is a well-known benchmark for testing tabular data modeling approaches. Let me share how I combined curiosity, random forests, and a splash of feature-importance introspection to beat all of the published leaderboard scores. Along the way, I’ll highlight the code and interpret the key insights so you can replicate (or improve upon!) my results with ease."
  },
  {
    "objectID": "posts/250317-adults-tabular-sota/07_tabular.html#introduction-and-setup",
    "href": "posts/250317-adults-tabular-sota/07_tabular.html#introduction-and-setup",
    "title": "A State-of-the-Art Random Forest: Beating the Leaderboard on Adult Census Data",
    "section": "Introduction and Setup",
    "text": "Introduction and Setup\nLet’s start by installing and importing all the right libraries. We’ll use kaggle, waterfallcharts, treeinterpreter, and dtreeviz to help interpret our models. Then we’ll bring in functionality from fastai to handle the dataset and tabular pipelines elegantly. Here’s the setup:\n\n\nCode\n! pip install kaggle waterfallcharts treeinterpreter dtreeviz==1.4.1\nimport fastbook\nfastbook.setup_book()\n\n\n\n\nCode\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\n\nWe’ll work with the Adult Sample version of the dataset, which is just enough data to be illustrative. Let’s download and peek at the dataset:\n\npath = untar_data(URLs.ADULT_SAMPLE)\n\n\npath.ls()\n\n(#3) [Path('/root/.fastai/data/adult_sample/models'),Path('/root/.fastai/data/adult_sample/export.pkl'),Path('/root/.fastai/data/adult_sample/adult.csv')]\n\n\n\ndf = pd.read_csv(path/'adult.csv', low_memory=False, skipinitialspace=True)\ndf.head()\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nsalary\n\n\n\n\n0\n49\nPrivate\n101320\nAssoc-acdm\n12.0\nMarried-civ-spouse\nNaN\nWife\nWhite\nFemale\n0\n1902\n40\nUnited-States\n&gt;=50k\n\n\n1\n44\nPrivate\n236746\nMasters\n14.0\nDivorced\nExec-managerial\nNot-in-family\nWhite\nMale\n10520\n0\n45\nUnited-States\n&gt;=50k\n\n\n2\n38\nPrivate\n96185\nHS-grad\nNaN\nDivorced\nNaN\nUnmarried\nBlack\nFemale\n0\n0\n32\nUnited-States\n&lt;50k\n\n\n3\n38\nSelf-emp-inc\n112847\nProf-school\n15.0\nMarried-civ-spouse\nProf-specialty\nHusband\nAsian-Pac-Islander\nMale\n0\n0\n40\nUnited-States\n&gt;=50k\n\n\n4\n42\nSelf-emp-not-inc\n82297\n7th-8th\nNaN\nMarried-civ-spouse\nOther-service\nWife\nBlack\nFemale\n0\n0\n50\nUnited-States\n&lt;50k\n\n\n\n\n\n\n\nYou should see basic demographic and earnings information. The outcome (salary) indicates whether a person makes over or under $50k per year.\n\nEarly Exploration\nIt’s always good to quickly review basic dataset facts, like how many unique categories are in each column. Then we prepare the categorical variables and define a few preprocessing steps:\n\ndep_var = 'salary'\ndf[dep_var].unique()\n\narray(['&gt;=50k', '&lt;50k'], dtype=object)\n\n\n\nmetrics=accuracy\n\n\ndf.nunique()\n\nage                  73\nworkclass             9\nfnlwgt            21648\neducation            16\neducation-num        16\nmarital-status        7\noccupation           15\nrelationship          6\nrace                  5\nsex                   2\ncapital-gain        119\ncapital-loss         92\nhours-per-week       94\nnative-country       42\nsalary                2\ndtype: int64\n\n\nObserving the output, we see a handful of numeric features, plus many categorical ones like workclass, education, and so on. The dataset uses “&lt;50k” and “&gt;=50k” for our binary classification target.\n\n\nTidying Up Categories\nWe set up the ‘education’ levels in an ordered fashion (from the least schooling to the highest), and do the same with salary levels:\n\neducation_levels = ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th', 'HS-grad', 'Assoc-voc', \n                    'Assoc-acdm', 'Some-college', 'Bachelors', 'Masters', 'Prof-school', 'Doctorate']\nsalary_levels = ['&lt;50k', '&gt;=50k']\n\n\nordinal_col = ['education', 'salary']\norders = [education_levels, salary_levels]\n\nfor i in zip(ordinal_col, orders):\n    x = i[0]\n    df[x] = df[x].astype('category')\n    df[x].cat.set_categories(i[1], ordered=True, inplace=True)\n\n\nprocs = [Categorify, FillMissing]\n\nWe then split off a validation set to ensure we can measure how well our model generalizes:\n\ntrain_idx, valid_idx = train_test_split(df.index, test_size=0.2, random_state=42)\nsplits = (list(train_idx),list(valid_idx))\n\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\nlen(to.train), len(to.valid)\nto.show(3)  # Inspect the TabularPandas in a reader-friendly way\nto.items.head(3)  # Inspect the underlying numerical data\n\n\n\n\n\nworkclass\neducation\nmarital-status\noccupation\nrelationship\nrace\nsex\nnative-country\neducation-num_na\nage\nfnlwgt\neducation-num\ncapital-gain\ncapital-loss\nhours-per-week\nsalary\n\n\n\n\n5514\nPrivate\n11th\nNever-married\nMachine-op-inspct\nNot-in-family\nWhite\nMale\nUnited-States\nFalse\n35\n267966\n7.0\n0\n0\n50\n&lt;50k\n\n\n19777\nPrivate\nAssoc-acdm\nSeparated\nCraft-repair\nUnmarried\nAsian-Pac-Islander\nMale\nUnited-States\nFalse\n24\n243190\n12.0\n8614\n0\n40\n&gt;=50k\n\n\n10781\nPrivate\nHS-grad\nDivorced\nMachine-op-inspct\nOther-relative\nWhite\nMale\nUnited-States\nFalse\n31\n191834\n9.0\n0\n0\n40\n&lt;50k\n\n\n\n\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nsalary\neducation-num_na\n\n\n\n\n5514\n35\n5\n267966\n7\n7.0\n5\n8\n2\n5\n2\n0\n0\n50\n40\n0\n1\n\n\n19777\n24\n5\n243190\n11\n12.0\n6\n4\n5\n2\n2\n8614\n0\n40\n40\n1\n1\n\n\n10781\n31\n5\n191834\n9\n9.0\n1\n8\n3\n5\n2\n0\n0\n40\n40\n0\n1\n\n\n\n\n\n\n\n\nsave_pickle(path/'to.pkl',to)"
  },
  {
    "objectID": "posts/250317-adults-tabular-sota/07_tabular.html#improving-from-baseline-feature-importance",
    "href": "posts/250317-adults-tabular-sota/07_tabular.html#improving-from-baseline-feature-importance",
    "title": "A State-of-the-Art Random Forest: Beating the Leaderboard on Adult Census Data",
    "section": "Improving from Baseline: Feature Importance",
    "text": "Improving from Baseline: Feature Importance\nRandom forests let us see which features matter most. Maybe we can prune out some weaker variables for improved accuracy or interpretability. For instance, if native-country or sex are near-zero in importance, we might drop them. Sometimes that increases accuracy (or at least simplifies the model for more robust generalization).\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\nfi = rf_feat_importance(m, xs)\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi);\n\n\n\n\n\n\n\n\nIt looks like maybe we can drop native-country, sex, race, and education-num_na. Let’s try.\n\nto_keep = fi[fi.imp&gt;0.015].cols\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\nm = rf(xs_imp, y)\npreds = m.predict(valid_xs_imp)\nprint(\"Accuracy:   \", acc(preds, valid_y), \"\\nOOB Score:   \", m.oob_score_)\n\nAccuracy:    0.860586519269154 \nOOB Score:    0.8599892506142506\n\n\nLook at that—similar or better accuracy, with fewer features. Simpler can be better!"
  },
  {
    "objectID": "posts/250317-adults-tabular-sota/07_tabular.html#redundant-features",
    "href": "posts/250317-adults-tabular-sota/07_tabular.html#redundant-features",
    "title": "A State-of-the-Art Random Forest: Beating the Leaderboard on Adult Census Data",
    "section": "Redundant Features",
    "text": "Redundant Features\nIf education and education-num capture the same essence, we can try dropping one. Check which yields the higher OOB score. In many experiments, I found dropping education left me with a better model:\n\ncluster_columns(xs_imp)\n\n\n\n\n\n\n\n\n\ndef get_oob(df):\n    m = RandomForestClassifier(n_jobs=-1, n_estimators=40,\n        max_samples=13000, max_features=0.5,\n        min_samples_leaf=5, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\nget_oob(xs_imp)\n\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'education-num', 'education')}\n\n{'education-num': 0.8609106265356266, 'education': 0.8620239557739557}\n\n\nGreat! We get a little boost if we drop education but keep education-num. Let’s do it.\n\nto_drop = ['education']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.8624078624078624\n\n\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\nsave_pickle(path/'xs_final.pkl', xs_final)\nsave_pickle(path/'valid_xs_final.pkl', valid_xs_final)\n\nAt the end of all this, our simpler model is still robust and obtains an accuracy around 86.2%. This already beats a standard published benchmark— woohoo!"
  },
  {
    "objectID": "posts/250703-coral-reef-threats/threats-to-coral-reefs.html",
    "href": "posts/250703-coral-reef-threats/threats-to-coral-reefs.html",
    "title": "Coal vs. Coral: Why Retiring the World’s Dirtiest Fuel Is Reef Conservation’s First Battle",
    "section": "",
    "text": "The people of the Pacific nation of Fiji have a lightness to their way of life, both materially and interpersonally. Home to an army of exceedingly gracious, why-don’t-you-stay-for-dinner types, the country has gained a reputation as the friendliest place on earth.\nTo illustrate the fact that this is true true, and not just good marketing true: when it became evident that their Pacific neighbors in Vanuatu were going to lose their country to sea level rise, Fiji offered one of its islands as a new home to the tens of thousands of people about to be displaced. Remarkable.\nAnd to illustrate the fact that Fijians are both gracious and self-effacing, when I asked one woman what she thought it was about her culture that caused them to have such a good-natured reputation, she laughed. “Well I think it’s because our ancestors used to eat people, and so now we want to make sure that nobody visiting here feels uncomfortable.”"
  },
  {
    "objectID": "posts/250703-coral-reef-threats/threats-to-coral-reefs.html#causes-of-reef-decline",
    "href": "posts/250703-coral-reef-threats/threats-to-coral-reefs.html#causes-of-reef-decline",
    "title": "Coal vs. Coral: Why Retiring the World’s Dirtiest Fuel Is Reef Conservation’s First Battle",
    "section": "Causes of Reef Decline",
    "text": "Causes of Reef Decline\n\nThe Global Quadruple Threat\nReef ecosystems across the entire globe are threatened by a quadruple-onslaught of rising ocean temperatures, decreasing pH, destructive fishing practices, and nutrient runoff. Warm water events in 2021 and 2022 saw a bleach or die off of 42% of the Great Barrier Reef in the first year and a further 30% the second.\nThis century, the ocean is on track to become 2.5 times more acidic than it was at the beginning of the last one, driven by the release of carbon dioxide into the atmosphere from the burning of fossil fuels. In this future ocean, corals and other creatures whose skeletons are made of a calcium carbonate will very definitely dissolve.\nWhile the seriousness of this situation is hard to overstate, there are major levers we can and are trying hard to pull to adjust course and preserve the incredible beauty, not to mention the inherent right to live, of an ecosystem that we ourselves badly need.\nBy far the greatest driver of both ocean acidification and warming waters are the greenhouse gases released by the worldwide energy sector, with electricity and heat production heavily dominating the total makeup of global emissions. In 2024, coal still supplied one third of all the electricity production worldwide. Natural gas supplied another 20%, with nuclear, hydropower, wind, and solar rounding out the rest.\n\n\n\nGlobal CO2 emissions by sector"
  },
  {
    "objectID": "posts/250703-coral-reef-threats/threats-to-coral-reefs.html#the-15-fix-what-replacing-coal-achieves",
    "href": "posts/250703-coral-reef-threats/threats-to-coral-reefs.html#the-15-fix-what-replacing-coal-achieves",
    "title": "Coal vs. Coral: Why Retiring the World’s Dirtiest Fuel Is Reef Conservation’s First Battle",
    "section": "The 15% Fix: What Replacing Coal Achieves",
    "text": "The 15% Fix: What Replacing Coal Achieves\nWhen a task is large, it can be hard to choose the right entry point for addressing it. The good news is that there’s plenty of work to go around, and whichever entry point you choose helps. In this series, we’ll be focusing on coal. When we replace all of the world’s coal-fired electricity and heat production with non-emitting alternatives, we will have eliminated 15% of the total problem of human-released CO2 and thus stepped something like 15% of the way along the path to allowing corals to continue to live on our planet.\nWe’ve created an inhospitable home to a very precious animal, one which underpins global foodstocks, biodiversity, and the economic health of a large portion of the world. The task at hand, and one which we will succeed at, it to provide coral with a healthy alternative to the world as it currently stands."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the Deep Blue with Deep Learning",
    "section": "",
    "text": "Coal vs. Coral: Why Retiring the World’s Dirtiest Fuel Is Reef Conservation’s First Battle\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 3, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\n\n\n\n\n\n\nULMFiT on AG News: Fine-Tuning a Wikipedia-Pretrained Model\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\n\n\n\n\n\n\nA State-of-the-Art Random Forest: Beating the Leaderboard on Adult Census Data\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\n\n\n\n\n\n\nSurpassing the Leaderboard: Building a #1 Ranking ResNet-18 on CIFAR-10\n\n\n\nnews\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 13, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\n\n\n\n\n\n\nLearning rates and mixed precision training\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying deep sea, reef, and freshwater fishes with a simple classifier\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nLindy Rauchenstein\n\n\n\n\n\nNo matching items"
  }
]